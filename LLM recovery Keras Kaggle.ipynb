{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f323c131",
   "metadata": {
    "papermill": {
     "duration": 0.012577,
     "end_time": "2024-04-15T08:22:12.883690",
     "exception": false,
     "start_time": "2024-04-15T08:22:12.871113",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import Libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fc0274",
   "metadata": {},
   "source": [
    "### Code follows the structure and training details of the Starter Notebook provided by Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2cbf909",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-04-15T08:22:12.906713Z",
     "iopub.status.busy": "2024-04-15T08:22:12.906429Z",
     "iopub.status.idle": "2024-04-15T08:22:27.376080Z",
     "shell.execute_reply": "2024-04-15T08:22:27.375286Z"
    },
    "papermill": {
     "duration": 14.483669,
     "end_time": "2024-04-15T08:22:27.378427",
     "exception": false,
     "start_time": "2024-04-15T08:22:12.894758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 08:22:17.712544: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-15 08:22:17.712639: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-15 08:22:17.855988: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\" # you can also use tensorflow or torch\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\" # avoid memory fragmentation on JAX backend.\n",
    "\n",
    "import keras\n",
    "import keras_nlp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas() # progress bar for pandas\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad849ba0",
   "metadata": {
    "papermill": {
     "duration": 0.010926,
     "end_time": "2024-04-15T08:22:27.400787",
     "exception": false,
     "start_time": "2024-04-15T08:22:27.389861",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cba78bad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-15T08:22:27.425662Z",
     "iopub.status.busy": "2024-04-15T08:22:27.425059Z",
     "iopub.status.idle": "2024-04-15T08:22:27.429867Z",
     "shell.execute_reply": "2024-04-15T08:22:27.429001Z"
    },
    "papermill": {
     "duration": 0.0187,
     "end_time": "2024-04-15T08:22:27.431749",
     "exception": false,
     "start_time": "2024-04-15T08:22:27.413049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    seed = 42\n",
    "    dataset_path = \"/kaggle/input/llm-prompt-recovery\"\n",
    "    preset = \"gemma_instruct_2b_en\" # name of pretrained Gemma\n",
    "    sequence_length = 512 # max size of input sequence for training\n",
    "    batch_size = 1 # size of the input batch in training\n",
    "    epochs = 3 # number of epochs to train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7981df5",
   "metadata": {
    "papermill": {
     "duration": 0.010831,
     "end_time": "2024-04-15T08:22:27.453489",
     "exception": false,
     "start_time": "2024-04-15T08:22:27.442658",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Reproducibility \n",
    "Sets value for random seed to produce similar result in each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91853929",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-15T08:22:27.476515Z",
     "iopub.status.busy": "2024-04-15T08:22:27.476232Z",
     "iopub.status.idle": "2024-04-15T08:22:27.480305Z",
     "shell.execute_reply": "2024-04-15T08:22:27.479489Z"
    },
    "papermill": {
     "duration": 0.01789,
     "end_time": "2024-04-15T08:22:27.482273",
     "exception": false,
     "start_time": "2024-04-15T08:22:27.464383",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c8e7bf",
   "metadata": {
    "papermill": {
     "duration": 0.010882,
     "end_time": "2024-04-15T08:22:27.504153",
     "exception": false,
     "start_time": "2024-04-15T08:22:27.493271",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data\n",
    "\n",
    "No training data is provided in this competition; in other words, we can use any openly available datasets for this competition. In this notebook, we will use two external datasets that utilize the **Gemma 7B** model to transform texts using prompts.\n",
    "\n",
    "**Data Format:**\n",
    "\n",
    "These datasets includes:\n",
    "- `original_text`: Input text/essay that needs to be transformed.\n",
    "- `rewrite_prompt`: Prompt/Instruction that was used in the Gemma LM to transform `original_text`. This is also our **target** for this competition.\n",
    "- `rewritten_text`: Output text that was generated by the Gemma model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3ce85d",
   "metadata": {
    "papermill": {
     "duration": 0.011338,
     "end_time": "2024-04-15T08:22:27.771396",
     "exception": false,
     "start_time": "2024-04-15T08:22:27.760058",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prompt Engineering\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0875043c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-15T08:22:27.824421Z",
     "iopub.status.busy": "2024-04-15T08:22:27.824149Z",
     "iopub.status.idle": "2024-04-15T08:22:27.827945Z",
     "shell.execute_reply": "2024-04-15T08:22:27.827153Z"
    },
    "papermill": {
     "duration": 0.017615,
     "end_time": "2024-04-15T08:22:27.829831",
     "exception": false,
     "start_time": "2024-04-15T08:22:27.812216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Use of Gemini prompts based on the technical report by Google's team\n",
    "\n",
    "gemini_prompts= \"\"\"<start_of_turn>user\n",
    "You are a reverse prompt engineer. Read the following \"Original Text\" and your task is to create the correct \"Instruction\" that instructs a LLM to generate the \"Rewriten Text\" accurately.\n",
    "\\n Original Text:\\n{original_text}\n",
    "\\n Rewriten Text:\\n{rewritten_text}\n",
    "<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\\nInstruction:\\n{rewrite_prompt}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecfd74e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-15T08:22:27.853445Z",
     "iopub.status.busy": "2024-04-15T08:22:27.853178Z",
     "iopub.status.idle": "2024-04-15T08:23:29.436887Z",
     "shell.execute_reply": "2024-04-15T08:23:29.435998Z"
    },
    "papermill": {
     "duration": 61.598159,
     "end_time": "2024-04-15T08:23:29.439203",
     "exception": false,
     "start_time": "2024-04-15T08:22:27.841044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25/200658009.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1_gem[\"prompt\"]= df1_gem.apply(lambda row: gemini_prompts.format(original_text=row.original_text,\n",
      "/tmp/ipykernel_25/200658009.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1_gem['token_count_promp'] = df1_gem[\"prompt\"].str.replace(',','').str.split().str.len()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#Counting the tokens to limit the inputs to 700 in training, so that it works with the memory provided by kaggle free GPU\n",
    "df_gem = pd.read_csv(\"/kaggle/input/all-in-one-dataset-with-embedding/df_with_emb.csv\")\n",
    "df1_gem= df_gem[[\"dataset_id\", \"original_text\", \"rewrite_prompt\",\"rewritten_text\"]]\n",
    "df1_gem[\"prompt\"]= df1_gem.apply(lambda row: gemini_prompts.format(original_text=row.original_text,\n",
    "                                                             rewritten_text=row.rewritten_text,\n",
    "                                                             rewrite_prompt=row.rewrite_prompt), axis=1)\n",
    "df1_gem['token_count_promp'] = df1_gem[\"prompt\"].str.replace(',','').str.split().str.len()\n",
    "final_df = df1_gem[df1_gem[\"token_count_promp\"]<=700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25eca4ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-15T08:23:29.463473Z",
     "iopub.status.busy": "2024-04-15T08:23:29.463172Z",
     "iopub.status.idle": "2024-04-15T08:23:29.476707Z",
     "shell.execute_reply": "2024-04-15T08:23:29.475823Z"
    },
    "papermill": {
     "duration": 0.027846,
     "end_time": "2024-04-15T08:23:29.478608",
     "exception": false,
     "start_time": "2024-04-15T08:23:29.450762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>original_text</th>\n",
       "      <th>rewrite_prompt</th>\n",
       "      <th>rewritten_text</th>\n",
       "      <th>prompt</th>\n",
       "      <th>token_count_promp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>host</td>\n",
       "      <td>Dear Randy,\\n\\nI hope this letter finds you we...</td>\n",
       "      <td>Rephrase this letter to infuse it with an elfi...</td>\n",
       "      <td>Dear Randy,\\n\\nMay this enchanted message find...</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\nYou are a reverse prompt ...</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nbroad_1</td>\n",
       "      <td>This quilt, that my mother made, \\n \\n Still m...</td>\n",
       "      <td>Regency Romance: Model the text on a Regency r...</td>\n",
       "      <td>The softest brown and brightest blue quilt, cr...</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\nYou are a reverse prompt ...</td>\n",
       "      <td>417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nbroad_1</td>\n",
       "      <td>It's the job of our agency to keep track of th...</td>\n",
       "      <td>Write like Ernest Hemingway: Focus on Hemingwa...</td>\n",
       "      <td>The agency's responsibility is to track and co...</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\nYou are a reverse prompt ...</td>\n",
       "      <td>943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nbroad_1</td>\n",
       "      <td>The first punch gets me right in the ribs, kno...</td>\n",
       "      <td>Grimm's Fairy Tales: Adapt the text to mimic t...</td>\n",
       "      <td>In the sweltering sun, the stench of sweat and...</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\nYou are a reverse prompt ...</td>\n",
       "      <td>603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nbroad_1</td>\n",
       "      <td>Some nights I lay awake staring at the ceiling...</td>\n",
       "      <td>High Fantasy Epic: Transform the essay into a ...</td>\n",
       "      <td>In the tapestry of the ethereal realm of Eldri...</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\nYou are a reverse prompt ...</td>\n",
       "      <td>1167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58169</th>\n",
       "      <td>aatiffraz</td>\n",
       "      <td>I knew I only had 10 seconds to change things....</td>\n",
       "      <td>Rewrite this text in the style of a philosophi...</td>\n",
       "      <td>\\n\\n## The Knight's Tale of Temporal Flux:\\n\\n...</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\nYou are a reverse prompt ...</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58170</th>\n",
       "      <td>aatiffraz</td>\n",
       "      <td>The rebels ran into the building lining up in ...</td>\n",
       "      <td>Restyle this text as if it were written by a p...</td>\n",
       "      <td>\\n\\nSure, here is the text rewritten as if it ...</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\nYou are a reverse prompt ...</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58171</th>\n",
       "      <td>aatiffraz</td>\n",
       "      <td>Jeanne loomed over the coffin, a great nagging...</td>\n",
       "      <td>Restyle this text as if it were written by a p...</td>\n",
       "      <td>\\n\\nSure, here is the text rewritten as if it ...</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\nYou are a reverse prompt ...</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58172</th>\n",
       "      <td>aatiffraz</td>\n",
       "      <td>The officers examined the graphic scene. The v...</td>\n",
       "      <td>Translate the essence of this text into a pira...</td>\n",
       "      <td>\\n\\n**Pirate Narrative:**\\n\\nAvast, me heartie...</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\nYou are a reverse prompt ...</td>\n",
       "      <td>268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58173</th>\n",
       "      <td>aatiffraz</td>\n",
       "      <td>Stacey ambulated across the bedroom towards me...</td>\n",
       "      <td>Convey the same message as this text but throu...</td>\n",
       "      <td>\\n\\nI have rewritten this text so that the mes...</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\nYou are a reverse prompt ...</td>\n",
       "      <td>329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58174 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      dataset_id                                      original_text  \\\n",
       "0           host  Dear Randy,\\n\\nI hope this letter finds you we...   \n",
       "1       nbroad_1  This quilt, that my mother made, \\n \\n Still m...   \n",
       "2       nbroad_1  It's the job of our agency to keep track of th...   \n",
       "3       nbroad_1  The first punch gets me right in the ribs, kno...   \n",
       "4       nbroad_1  Some nights I lay awake staring at the ceiling...   \n",
       "...          ...                                                ...   \n",
       "58169  aatiffraz  I knew I only had 10 seconds to change things....   \n",
       "58170  aatiffraz  The rebels ran into the building lining up in ...   \n",
       "58171  aatiffraz  Jeanne loomed over the coffin, a great nagging...   \n",
       "58172  aatiffraz  The officers examined the graphic scene. The v...   \n",
       "58173  aatiffraz  Stacey ambulated across the bedroom towards me...   \n",
       "\n",
       "                                          rewrite_prompt  \\\n",
       "0      Rephrase this letter to infuse it with an elfi...   \n",
       "1      Regency Romance: Model the text on a Regency r...   \n",
       "2      Write like Ernest Hemingway: Focus on Hemingwa...   \n",
       "3      Grimm's Fairy Tales: Adapt the text to mimic t...   \n",
       "4      High Fantasy Epic: Transform the essay into a ...   \n",
       "...                                                  ...   \n",
       "58169  Rewrite this text in the style of a philosophi...   \n",
       "58170  Restyle this text as if it were written by a p...   \n",
       "58171  Restyle this text as if it were written by a p...   \n",
       "58172  Translate the essence of this text into a pira...   \n",
       "58173  Convey the same message as this text but throu...   \n",
       "\n",
       "                                          rewritten_text  \\\n",
       "0      Dear Randy,\\n\\nMay this enchanted message find...   \n",
       "1      The softest brown and brightest blue quilt, cr...   \n",
       "2      The agency's responsibility is to track and co...   \n",
       "3      In the sweltering sun, the stench of sweat and...   \n",
       "4      In the tapestry of the ethereal realm of Eldri...   \n",
       "...                                                  ...   \n",
       "58169  \\n\\n## The Knight's Tale of Temporal Flux:\\n\\n...   \n",
       "58170  \\n\\nSure, here is the text rewritten as if it ...   \n",
       "58171  \\n\\nSure, here is the text rewritten as if it ...   \n",
       "58172  \\n\\n**Pirate Narrative:**\\n\\nAvast, me heartie...   \n",
       "58173  \\n\\nI have rewritten this text so that the mes...   \n",
       "\n",
       "                                                  prompt  token_count_promp  \n",
       "0      <start_of_turn>user\\nYou are a reverse prompt ...                202  \n",
       "1      <start_of_turn>user\\nYou are a reverse prompt ...                417  \n",
       "2      <start_of_turn>user\\nYou are a reverse prompt ...                943  \n",
       "3      <start_of_turn>user\\nYou are a reverse prompt ...                603  \n",
       "4      <start_of_turn>user\\nYou are a reverse prompt ...               1167  \n",
       "...                                                  ...                ...  \n",
       "58169  <start_of_turn>user\\nYou are a reverse prompt ...                340  \n",
       "58170  <start_of_turn>user\\nYou are a reverse prompt ...                320  \n",
       "58171  <start_of_turn>user\\nYou are a reverse prompt ...                349  \n",
       "58172  <start_of_turn>user\\nYou are a reverse prompt ...                268  \n",
       "58173  <start_of_turn>user\\nYou are a reverse prompt ...                329  \n",
       "\n",
       "[58174 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_gem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc497984",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-15T08:23:29.503351Z",
     "iopub.status.busy": "2024-04-15T08:23:29.502801Z",
     "iopub.status.idle": "2024-04-15T08:23:29.508632Z",
     "shell.execute_reply": "2024-04-15T08:23:29.507864Z"
    },
    "papermill": {
     "duration": 0.020036,
     "end_time": "2024-04-15T08:23:29.510512",
     "exception": false,
     "start_time": "2024-04-15T08:23:29.490476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = final_df.prompt.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "727cc3bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-15T08:23:29.535332Z",
     "iopub.status.busy": "2024-04-15T08:23:29.535030Z",
     "iopub.status.idle": "2024-04-15T08:23:29.540269Z",
     "shell.execute_reply": "2024-04-15T08:23:29.539423Z"
    },
    "papermill": {
     "duration": 0.019667,
     "end_time": "2024-04-15T08:23:29.542113",
     "exception": false,
     "start_time": "2024-04-15T08:23:29.522446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54379"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6bde2f",
   "metadata": {
    "papermill": {
     "duration": 0.011801,
     "end_time": "2024-04-15T08:23:29.621044",
     "exception": false,
     "start_time": "2024-04-15T08:23:29.609243",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83517e5a",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-04-15T08:23:29.645951Z",
     "iopub.status.busy": "2024-04-15T08:23:29.645680Z",
     "iopub.status.idle": "2024-04-15T08:23:29.650435Z",
     "shell.execute_reply": "2024-04-15T08:23:29.649540Z"
    },
    "papermill": {
     "duration": 0.019233,
     "end_time": "2024-04-15T08:23:29.652313",
     "exception": false,
     "start_time": "2024-04-15T08:23:29.633080",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def colorize_text(text):\n",
    "    for word, color in zip([\"Instruction\", \"Original Text\", \"Rewriten Text\", \"Response\"],\n",
    "                           [\"red\", \"yellow\", \"blue\", \"green\"]):\n",
    "        text = text.replace(f\"{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8855e340",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-04-15T08:23:29.677464Z",
     "iopub.status.busy": "2024-04-15T08:23:29.677200Z",
     "iopub.status.idle": "2024-04-15T08:23:29.683054Z",
     "shell.execute_reply": "2024-04-15T08:23:29.682309Z"
    },
    "papermill": {
     "duration": 0.020448,
     "end_time": "2024-04-15T08:23:29.684818",
     "exception": false,
     "start_time": "2024-04-15T08:23:29.664370",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<start_of_turn>user\n",
       "You are a reverse prompt engineer. Read the following \"Original Text\" and your task is to create the correct \"Instruction\" that instructs a LLM to generate the \"Rewriten Text\" accurately.\n",
       "\n",
       " \n",
       "\n",
       "**<font color='yellow'>Original Text:</font>**\n",
       "`` Happy Birthday, Lucifer.'' \n",
       " \n",
       " I hunched my shoulders, closed my eyes and took a deep breath. Only one person knew it was the anniversary of my creation and that was my creator. \n",
       " \n",
       " `` Thank you'' I turned to look at him, Him, the Almighty God. He had n't changed one bit. His presence filled me in a way my eons of sin never did, I felt whole.'' \n",
       " \n",
       " `` I have a gift for you Lucifer, but you have to choose, you can come home or...'' \n",
       " \n",
       " `` Or? \n",
       " \n",
       " `` I will admit that I was wrong, that you were right when you left. You can only have one. Which is it?''\n",
       "\n",
       "\n",
       " \n",
       "\n",
       "**<font color='blue'>Rewriten Text:</font>**\n",
       "The clock struck midnight, the wind whipped through the crumbling Victorian tower, and a cold, solitary figure stood in the center of the room. A deep, resonant howl echoed through the corridors, as the person hunched their shoulders, closed their eyes and took a deep breath. Only one person knew it was the anniversary of their creation and that was their creator.\n",
       "\n",
       "\"Happy Birthday, Lucifer,\" the person whispered into the night, their voice dripping in honeyed despair. They turned to look at their creator, the Almighty God, who stood tall in the dim light, unyielding and serene. His presence filled the person with a sense of wholeness, a feeling they had not experienced in their eons of sin.\n",
       "\n",
       "\"Thank you,\" the person said, their voice breaking. \"I have a gift for you, Lucifer, but you have to choose. You can come home or...\"\n",
       "\n",
       "A pause, a lingering look at the Almighty God, and then the person spoke again.\n",
       "\n",
       "\"Or,\" they breathed, their voice trailing off into the night. \"I will admit that I was wrong, that you were right when you left. You can only have one. Which is it?\"\n",
       "<end_of_turn>\n",
       "<start_of_turn>model\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Instruction:</font>**\n",
       "Victorian Gothic: Adopt a Victorian Gothic style, emphasizing ornate language, emotional intensity, and possibly supernatural elements.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Take a random sample\n",
    "sample = data[10]\n",
    "\n",
    "# Give colors to Instruction, Response and Category\n",
    "sample = colorize_text(sample)\n",
    "\n",
    "# Show sample in markdown\n",
    "display(Markdown(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa335d0",
   "metadata": {
    "papermill": {
     "duration": 0.011931,
     "end_time": "2024-04-15T08:23:29.708849",
     "exception": false,
     "start_time": "2024-04-15T08:23:29.696918",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Modeling\n",
    "\n",
    "<div align=\"center\"><img src=\"https://i.ibb.co/Bqg9w3g/Gemma-Logo-no-background.png\" width=\"300\"></div>\n",
    "\n",
    "**Gemma** is a collection of advanced open LLMs developed by **Google DeepMind** and other **Google teams**, derived from the same research and technology behind the **Gemini** models. They can be integrated into applications and run on various platforms including mobile devices and hosted services. Developers can customize Gemma models using tuning techniques to enhance their performance for specific tasks, offering more targeted and efficient generative AI solutions beyond text generation.\n",
    "\n",
    "Gemma models are available in several sizes so we can build generative AI solutions based on your available computing resources, the capabilities you need, and where you want to run them.\n",
    "\n",
    "| Parameters size | Tuned versions    | Intended platforms                 | Preset                 |\n",
    "|-----------------|-------------------|------------------------------------|------------------------|\n",
    "| 2B              | Pretrained        | Mobile devices and laptops         | `gemma_2b_en`          |\n",
    "| 2B              | Instruction tuned | Mobile devices and laptops         | `gemma_instruct_2b_en` |\n",
    "| 7B              | Pretrained        | Desktop computers and small servers| `gemma_7b_en`          |\n",
    "| 7B              | Instruction tuned | Desktop computers and small servers| `gemma_instruct_7b_en` |\n",
    "\n",
    "In this notebook, we will utilize the `Gemma 2b-it` model from KerasNLP's pretrained models to recover the prompt. We are using the \"Instruction tuned\" model instead of the \"Pretrained\" one because the test data was generated from an instruction-tuned Gemma model. Additionally, we will fine-tune our model using instruction-response pairs thus fine-tuning an instruction-tuned model will likely yield better results.\n",
    "\n",
    "To explore other available models, you can simply adjust the `preset` value in the `CFG` (config). You can find a list of other pretrained models on the [KerasNLP website](https://keras.io/api/keras_nlp/models/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561aedb8",
   "metadata": {
    "papermill": {
     "duration": 0.011995,
     "end_time": "2024-04-15T08:23:29.732939",
     "exception": false,
     "start_time": "2024-04-15T08:23:29.720944",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Gemma Causal LM\n",
    "\n",
    "The code below will build an end-to-end Gemma model for causal language modeling (hence the name `GemmaCausalLM`). A causal language model (LM) predicts the next token based on previous tokens. This task setup can be used to train the model unsupervised on plain text input or to autoregressively generate plain text similar to the data used for training. This task can be used for pre-training or fine-tuning a Gemma model simply by calling `fit()`.\n",
    "\n",
    "This model has a `generate()` method, which generates text based on a prompt. The generation strategy used is controlled by an additional sampler argument on `compile()`. You can recompile the model with different `keras_nlp.samplers` objects to control the generation. By default, `\"greedy\"` sampling will be used.\n",
    "\n",
    "> The `from_preset` method instantiates the model from a preset architecture and weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e37a6c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-15T08:23:29.758097Z",
     "iopub.status.busy": "2024-04-15T08:23:29.757808Z",
     "iopub.status.idle": "2024-04-15T08:24:32.027803Z",
     "shell.execute_reply": "2024-04-15T08:24:32.026914Z"
    },
    "papermill": {
     "duration": 62.284854,
     "end_time": "2024-04-15T08:24:32.029830",
     "exception": false,
     "start_time": "2024-04-15T08:23:29.744976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attaching 'config.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'config.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'model.weights.h5' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\n",
      "Attaching 'assets/tokenizer/vocabulary.spm' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,506,172,416\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(CFG.preset)\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b220ba44",
   "metadata": {
    "papermill": {
     "duration": 0.013737,
     "end_time": "2024-04-15T08:24:32.057525",
     "exception": false,
     "start_time": "2024-04-15T08:24:32.043788",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Gemma LM Preprocessor\n",
    "\n",
    "An important part of the Gemma model is the **Preprocessor** layer, which under the hood uses **Tokenizer**.\n",
    "\n",
    "**What it does:** The preprocessor takes input strings and transforms them into a dictionary (`token_ids`, `padding_mask`) containing preprocessed tensors. This process starts with tokenization, where input strings are converted into sequences of token IDs.\n",
    "\n",
    "**Why it's important:** Initially, raw text data is complex and challenging for modeling due to its high dimensionality. By converting text into a compact set of tokens, such as transforming `\"The quick brown fox\"` into `[\"the\", \"qu\", \"##ick\", \"br\", \"##own\", \"fox\"]`, we simplify the data. Many models rely on special tokens and additional tensors to understand input. These tokens help divide input and identify padding, among other tasks. Making all sequences the same length through padding boosts computational efficiency, making subsequent steps smoother.\n",
    "\n",
    "Explore the following pages to access the available preprocessing and tokenizer layers in **KerasNLP**:\n",
    "- [Preprocessing](https://keras.io/api/keras_nlp/preprocessing_layers/)\n",
    "- [Tokenizers](https://keras.io/api/keras_nlp/tokenizers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da88fa06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-15T08:24:32.087036Z",
     "iopub.status.busy": "2024-04-15T08:24:32.086344Z",
     "iopub.status.idle": "2024-04-15T08:24:32.412286Z",
     "shell.execute_reply": "2024-04-15T08:24:32.411480Z"
    },
    "papermill": {
     "duration": 0.343215,
     "end_time": "2024-04-15T08:24:32.414474",
     "exception": false,
     "start_time": "2024-04-15T08:24:32.071259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x, y, sample_weight = gemma_lm.preprocessor(data[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570f67c6",
   "metadata": {
    "papermill": {
     "duration": 0.013705,
     "end_time": "2024-04-15T08:24:32.485272",
     "exception": false,
     "start_time": "2024-04-15T08:24:32.471567",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This preprocessing layer will take in batches of strings, and return outputs in a `(x, y, sample_weight)` format, where the `y` label is the next token id in the `x` sequence.\n",
    "\n",
    "From the code below, we can see that, after the preprocessor, the data shape is `(num_samples, sequence_length)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dc483c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-15T08:24:32.514823Z",
     "iopub.status.busy": "2024-04-15T08:24:32.514158Z",
     "iopub.status.idle": "2024-04-15T08:24:32.518986Z",
     "shell.execute_reply": "2024-04-15T08:24:32.518150Z"
    },
    "papermill": {
     "duration": 0.021974,
     "end_time": "2024-04-15T08:24:32.521215",
     "exception": false,
     "start_time": "2024-04-15T08:24:32.499241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_ids : (2, 8192)\n",
      "padding_mask : (2, 8192)\n"
     ]
    }
   ],
   "source": [
    "# Display the shape of each processed output\n",
    "for k, v in x.items():\n",
    "    print(k, \":\", v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a93fa2",
   "metadata": {
    "papermill": {
     "duration": 0.013676,
     "end_time": "2024-04-15T08:24:32.548580",
     "exception": false,
     "start_time": "2024-04-15T08:24:32.534904",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference before Fine-Tuning\n",
    "\n",
    "Before we do fine-tuning, let's try to recover the prompt using the Gemma model with some prepared prompts and see how it responds.\n",
    "\n",
    "> As this model is not yet fine-tuned for instruction, you will notice that the model's responses are inaccurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3d1115",
   "metadata": {
    "papermill": {
     "duration": 0.013512,
     "end_time": "2024-04-15T08:24:32.575868",
     "exception": false,
     "start_time": "2024-04-15T08:24:32.562356",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Fine-tuning with LoRA\n",
    "\n",
    "To get better responses from the model, we will fine-tune the model with Low Rank Adaptation (LoRA).\n",
    "\n",
    "**What exactly is LoRA?**\n",
    "\n",
    "LoRA is a method used to fine-tune large language models (LLMs) in an efficient way. It involves freezing the weights of the LLM and injecting trainable rank-decomposition matrices.\n",
    "\n",
    "Imagine in an LLM, we have a pre-trained dense layer, represented by a $d \\times d$ weight matrix, denoted as $W_0$. We then initialize two additional dense layers, labeled as $A$ and $B$, with shapes $d \\times r$ and $r \\times d$, respectively. Here, $r$ denotes the rank, which is typically **much smaller than** $d$. Prior to LoRA, the model's output was computed using the equation $output = W_0 \\cdot x + b_0$, where $x$ represents the input and $b_0$ denotes the bias term associated with the original dense layer, which remains frozen. After applying LoRA, the equation becomes $output = (W_0 \\cdot x + b_0) + (B \\cdot A \\cdot x)$, where $A$ and $B$ denote the trainable rank-decomposition matrices that have been introduced.\n",
    "\n",
    "<center><img src=\"https://i.ibb.co/DWsbhLg/LoRA.png\" width=\"300\"><br/>\n",
    "Credit: <a href=\"https://arxiv.org/abs/2106.09685\">LoRA: Low-Rank Adaptation of Large Language Models</a> Paper</center>\n",
    "\n",
    "\n",
    "In the LoRA paper, $A$ is initialized with $\\mathcal{N} (0, \\sigma^2)$ and $B$ with $0$, where $\\mathcal{N}$ denotes the normal distribution, and $\\sigma^2$ is the variance.\n",
    "\n",
    "**Why does LoRA save memory?**\n",
    "\n",
    "Even though we're adding more layers to the model with LoRA, it actually helps save memory. This is because the smaller layers (A and B) have fewer parameters to learn compared to the big model and fewer trainable parameters mean fewer optimizer variables to store. So, even though the overall model might seem bigger, it's actually more efficient in terms of memory usage. \n",
    "\n",
    "> This notebook uses a LoRA rank of `4`. A higher rank means more detailed changes are possible, but also means more trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a89d31b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-15T08:24:32.604715Z",
     "iopub.status.busy": "2024-04-15T08:24:32.604445Z",
     "iopub.status.idle": "2024-04-15T08:24:33.050029Z",
     "shell.execute_reply": "2024-04-15T08:24:33.049149Z"
    },
    "papermill": {
     "duration": 0.462372,
     "end_time": "2024-04-15T08:24:33.052073",
     "exception": false,
     "start_time": "2024-04-15T08:24:32.589701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,507,536,384\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,507,536,384\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,363,968</span> (5.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,363,968\u001b[0m (5.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Enable LoRA for the model and set the LoRA rank to 4.\n",
    "gemma_lm.backbone.enable_lora(rank=4)\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa71dd0a",
   "metadata": {
    "papermill": {
     "duration": 0.014699,
     "end_time": "2024-04-15T08:24:33.082135",
     "exception": false,
     "start_time": "2024-04-15T08:24:33.067436",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Notice** that, the number of trainable parameters is reduced from ~$2.5$ billions to ~$1.3$ millions after enabling LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd54d1c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-15T08:24:33.113280Z",
     "iopub.status.busy": "2024-04-15T08:24:33.112803Z",
     "iopub.status.idle": "2024-04-15T08:24:33.116852Z",
     "shell.execute_reply": "2024-04-15T08:24:33.116041Z"
    },
    "papermill": {
     "duration": 0.021786,
     "end_time": "2024-04-15T08:24:33.118733",
     "exception": false,
     "start_time": "2024-04-15T08:24:33.096947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data= data[:3000]\n",
    "#truncating data to 3000 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b5de5d",
   "metadata": {
    "papermill": {
     "duration": 0.014624,
     "end_time": "2024-04-15T08:24:33.148394",
     "exception": false,
     "start_time": "2024-04-15T08:24:33.133770",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6de9b8",
   "metadata": {},
   "source": [
    "Two schedulers are presented here. \n",
    "\n",
    "\n",
    "-ReduceLROnPlateau: This callback monitors a quantity and if no improvement is seen for a 'patience' number of epochs, the learning rate is reduced.\n",
    "\n",
    "\n",
    "-CosineDecay: A LearningRateSchedule that uses a cosine decay with optional warmup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97b462be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-15T08:24:33.179398Z",
     "iopub.status.busy": "2024-04-15T08:24:33.179140Z",
     "iopub.status.idle": "2024-04-15T10:59:51.252498Z",
     "shell.execute_reply": "2024-04-15T10:59:51.251405Z"
    },
    "papermill": {
     "duration": 9318.091101,
     "end_time": "2024-04-15T10:59:51.254507",
     "exception": false,
     "start_time": "2024-04-15T08:24:33.163406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3120s\u001b[0m 1s/step - loss: 2.1128 - sparse_categorical_accuracy: 0.5331\n",
      "Epoch 2/3\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3098s\u001b[0m 1s/step - loss: 1.8380 - sparse_categorical_accuracy: 0.5652\n",
      "Epoch 3/3\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3098s\u001b[0m 1s/step - loss: 1.8221 - sparse_categorical_accuracy: 0.5667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7dec034fb460>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Limit the input sequence length to 512 (to control memory usage).\n",
    "gemma_lm.preprocessor.sequence_length = 700#CFG.sequence_length \n",
    "\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='loss', factor=0.2,\n",
    "                              patience=5, min_lr=0.001\n",
    "\n",
    ")\n",
    "# Learning Rate\n",
    "total_steps = 3000*CFG.epochs\n",
    "decay_steps = total_steps * 0.7\n",
    "\n",
    "cosine_decay_scheduler = keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate = 3e-5, decay_steps= decay_steps, alpha=0.1\n",
    ")\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=cosine_decay_scheduler)\n",
    "\n",
    "# Compile the model with loss, optimizer, and metric\n",
    "gemma_lm.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=cosine_decay_scheduler),\n",
    "    #optimizer=keras.optimizers.Adam(learning_rate=3e-5),\n",
    "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "# Train model\n",
    "gemma_lm.fit(test_data, epochs=CFG.epochs, batch_size=CFG.batch_size,)# if using reduce on plateou add this line: callbacks=[reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cccc623",
   "metadata": {
    "papermill": {
     "duration": 0.750557,
     "end_time": "2024-04-15T10:59:52.761357",
     "exception": false,
     "start_time": "2024-04-15T10:59:52.010800",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference after fine-tuning\n",
    "\n",
    "Let's see how our fine-tuned model responds to the same questions we asked before fine-tuning the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b55a046",
   "metadata": {
    "papermill": {
     "duration": 0.746559,
     "end_time": "2024-04-15T10:59:54.322517",
     "exception": false,
     "start_time": "2024-04-15T10:59:53.575958",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Sample 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bda624f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-15T10:59:55.802937Z",
     "iopub.status.busy": "2024-04-15T10:59:55.802034Z",
     "iopub.status.idle": "2024-04-15T10:59:55.816657Z",
     "shell.execute_reply": "2024-04-15T10:59:55.815800Z"
    },
    "papermill": {
     "duration": 0.758346,
     "end_time": "2024-04-15T10:59:55.818557",
     "exception": false,
     "start_time": "2024-04-15T10:59:55.060211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>original_text</th>\n",
       "      <th>rewrite_prompt</th>\n",
       "      <th>rewritten_text</th>\n",
       "      <th>prompt</th>\n",
       "      <th>token_count_promp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>host</td>\n",
       "      <td>Dear Randy,\\n\\nI hope this letter finds you we...</td>\n",
       "      <td>Rephrase this letter to infuse it with an elfi...</td>\n",
       "      <td>Dear Randy,\\n\\nMay this enchanted message find...</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\nYou are a reverse prompt ...</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nbroad_1</td>\n",
       "      <td>This quilt, that my mother made, \\n \\n Still m...</td>\n",
       "      <td>Regency Romance: Model the text on a Regency r...</td>\n",
       "      <td>The softest brown and brightest blue quilt, cr...</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\nYou are a reverse prompt ...</td>\n",
       "      <td>417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nbroad_1</td>\n",
       "      <td>It's the job of our agency to keep track of th...</td>\n",
       "      <td>Write like Ernest Hemingway: Focus on Hemingwa...</td>\n",
       "      <td>The agency's responsibility is to track and co...</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\nYou are a reverse prompt ...</td>\n",
       "      <td>943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nbroad_1</td>\n",
       "      <td>The first punch gets me right in the ribs, kno...</td>\n",
       "      <td>Grimm's Fairy Tales: Adapt the text to mimic t...</td>\n",
       "      <td>In the sweltering sun, the stench of sweat and...</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\nYou are a reverse prompt ...</td>\n",
       "      <td>603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nbroad_1</td>\n",
       "      <td>Some nights I lay awake staring at the ceiling...</td>\n",
       "      <td>High Fantasy Epic: Transform the essay into a ...</td>\n",
       "      <td>In the tapestry of the ethereal realm of Eldri...</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\nYou are a reverse prompt ...</td>\n",
       "      <td>1167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58169</th>\n",
       "      <td>aatiffraz</td>\n",
       "      <td>I knew I only had 10 seconds to change things....</td>\n",
       "      <td>Rewrite this text in the style of a philosophi...</td>\n",
       "      <td>\\n\\n## The Knight's Tale of Temporal Flux:\\n\\n...</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\nYou are a reverse prompt ...</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58170</th>\n",
       "      <td>aatiffraz</td>\n",
       "      <td>The rebels ran into the building lining up in ...</td>\n",
       "      <td>Restyle this text as if it were written by a p...</td>\n",
       "      <td>\\n\\nSure, here is the text rewritten as if it ...</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\nYou are a reverse prompt ...</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58171</th>\n",
       "      <td>aatiffraz</td>\n",
       "      <td>Jeanne loomed over the coffin, a great nagging...</td>\n",
       "      <td>Restyle this text as if it were written by a p...</td>\n",
       "      <td>\\n\\nSure, here is the text rewritten as if it ...</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\nYou are a reverse prompt ...</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58172</th>\n",
       "      <td>aatiffraz</td>\n",
       "      <td>The officers examined the graphic scene. The v...</td>\n",
       "      <td>Translate the essence of this text into a pira...</td>\n",
       "      <td>\\n\\n**Pirate Narrative:**\\n\\nAvast, me heartie...</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\nYou are a reverse prompt ...</td>\n",
       "      <td>268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58173</th>\n",
       "      <td>aatiffraz</td>\n",
       "      <td>Stacey ambulated across the bedroom towards me...</td>\n",
       "      <td>Convey the same message as this text but throu...</td>\n",
       "      <td>\\n\\nI have rewritten this text so that the mes...</td>\n",
       "      <td>&lt;start_of_turn&gt;user\\nYou are a reverse prompt ...</td>\n",
       "      <td>329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58174 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      dataset_id                                      original_text  \\\n",
       "0           host  Dear Randy,\\n\\nI hope this letter finds you we...   \n",
       "1       nbroad_1  This quilt, that my mother made, \\n \\n Still m...   \n",
       "2       nbroad_1  It's the job of our agency to keep track of th...   \n",
       "3       nbroad_1  The first punch gets me right in the ribs, kno...   \n",
       "4       nbroad_1  Some nights I lay awake staring at the ceiling...   \n",
       "...          ...                                                ...   \n",
       "58169  aatiffraz  I knew I only had 10 seconds to change things....   \n",
       "58170  aatiffraz  The rebels ran into the building lining up in ...   \n",
       "58171  aatiffraz  Jeanne loomed over the coffin, a great nagging...   \n",
       "58172  aatiffraz  The officers examined the graphic scene. The v...   \n",
       "58173  aatiffraz  Stacey ambulated across the bedroom towards me...   \n",
       "\n",
       "                                          rewrite_prompt  \\\n",
       "0      Rephrase this letter to infuse it with an elfi...   \n",
       "1      Regency Romance: Model the text on a Regency r...   \n",
       "2      Write like Ernest Hemingway: Focus on Hemingwa...   \n",
       "3      Grimm's Fairy Tales: Adapt the text to mimic t...   \n",
       "4      High Fantasy Epic: Transform the essay into a ...   \n",
       "...                                                  ...   \n",
       "58169  Rewrite this text in the style of a philosophi...   \n",
       "58170  Restyle this text as if it were written by a p...   \n",
       "58171  Restyle this text as if it were written by a p...   \n",
       "58172  Translate the essence of this text into a pira...   \n",
       "58173  Convey the same message as this text but throu...   \n",
       "\n",
       "                                          rewritten_text  \\\n",
       "0      Dear Randy,\\n\\nMay this enchanted message find...   \n",
       "1      The softest brown and brightest blue quilt, cr...   \n",
       "2      The agency's responsibility is to track and co...   \n",
       "3      In the sweltering sun, the stench of sweat and...   \n",
       "4      In the tapestry of the ethereal realm of Eldri...   \n",
       "...                                                  ...   \n",
       "58169  \\n\\n## The Knight's Tale of Temporal Flux:\\n\\n...   \n",
       "58170  \\n\\nSure, here is the text rewritten as if it ...   \n",
       "58171  \\n\\nSure, here is the text rewritten as if it ...   \n",
       "58172  \\n\\n**Pirate Narrative:**\\n\\nAvast, me heartie...   \n",
       "58173  \\n\\nI have rewritten this text so that the mes...   \n",
       "\n",
       "                                                  prompt  token_count_promp  \n",
       "0      <start_of_turn>user\\nYou are a reverse prompt ...                202  \n",
       "1      <start_of_turn>user\\nYou are a reverse prompt ...                417  \n",
       "2      <start_of_turn>user\\nYou are a reverse prompt ...                943  \n",
       "3      <start_of_turn>user\\nYou are a reverse prompt ...                603  \n",
       "4      <start_of_turn>user\\nYou are a reverse prompt ...               1167  \n",
       "...                                                  ...                ...  \n",
       "58169  <start_of_turn>user\\nYou are a reverse prompt ...                340  \n",
       "58170  <start_of_turn>user\\nYou are a reverse prompt ...                320  \n",
       "58171  <start_of_turn>user\\nYou are a reverse prompt ...                349  \n",
       "58172  <start_of_turn>user\\nYou are a reverse prompt ...                268  \n",
       "58173  <start_of_turn>user\\nYou are a reverse prompt ...                329  \n",
       "\n",
       "[58174 rows x 6 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_gem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de98ca81",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-04-15T10:59:57.378844Z",
     "iopub.status.busy": "2024-04-15T10:59:57.378453Z",
     "iopub.status.idle": "2024-04-15T11:00:10.720444Z",
     "shell.execute_reply": "2024-04-15T11:00:10.719400Z"
    },
    "papermill": {
     "duration": 14.095061,
     "end_time": "2024-04-15T11:00:10.722486",
     "exception": false,
     "start_time": "2024-04-15T10:59:56.627425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<start_of_turn>user\n",
       "You are a reverse prompt engineer. Read the following \"Original Text\" and your task is to create the correct \"Instruction\" that instructs a LLM to generate the \"Rewriten Text\" accurately.\n",
       "\n",
       " \n",
       "\n",
       "**<font color='yellow'>Original Text:</font>**\n",
       "Dear Randy,\\n\\nI hope this letter finds you well. I wanted to reach out and let you know about a new styling service we are offering at our salon, called \"Trendy Styles.\" Our experienced stylists are skilled in creating the latest trends and can help you achieve the perfect look. Book an appointment today and let us help you elevate your style!\\n\\nBest regards,\\n[Your Name]\n",
       "\n",
       " \n",
       "\n",
       "**<font color='blue'>Rewriten Text:</font>**\n",
       "Dear Randy,\\n\\nMay this enchanted message find you in the most delightful of spirits. I simply had to share tidings of a wondrous service now gracing our humble salon, known as \"Trendy Styles.\" Our gifted stylists possess a magical touch, conjuring the latest trends to bestow upon you the most exquisite visage. Cast your appointment spell today and allow us to weave enchantment into your style.\\n\\nWarm regards,\\n[Your Name]\n",
       "<end_of_turn>\n",
       "<start_of_turn>model\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Instruction:</font>**\n",
       "\n",
       "Rewrite this as a magical message from the salon.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Take one sample\n",
    "row = df1_gem.iloc[0]\n",
    "\n",
    "# Generate Prompt using template\n",
    "prompt = gemini_prompts.format(\n",
    "    original_text=row.original_text,\n",
    "    rewritten_text=row.rewritten_text,\n",
    "    rewrite_prompt=\"\",\n",
    ")\n",
    "\n",
    "# Infer\n",
    "output = gemma_lm.generate(prompt, max_length=512)\n",
    "\n",
    "# Colorize\n",
    "output = colorize_text(output)\n",
    "\n",
    "# Display in markdown\n",
    "display(Markdown(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546b607a",
   "metadata": {
    "papermill": {
     "duration": 0.814396,
     "end_time": "2024-04-15T11:00:12.280714",
     "exception": false,
     "start_time": "2024-04-15T11:00:11.466318",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Sample 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be161c06",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-04-15T11:00:13.775913Z",
     "iopub.status.busy": "2024-04-15T11:00:13.775163Z",
     "iopub.status.idle": "2024-04-15T11:00:14.075053Z",
     "shell.execute_reply": "2024-04-15T11:00:14.074138Z"
    },
    "papermill": {
     "duration": 1.052939,
     "end_time": "2024-04-15T11:00:14.077425",
     "exception": false,
     "start_time": "2024-04-15T11:00:13.024486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<start_of_turn>user\n",
       "You are a reverse prompt engineer. Read the following \"Original Text\" and your task is to create the correct \"Instruction\" that instructs a LLM to generate the \"Rewriten Text\" accurately.\n",
       "\n",
       " \n",
       "\n",
       "**<font color='yellow'>Original Text:</font>**\n",
       "This quilt, that my mother made, \n",
       " \n",
       " Still makes me think to this day. \n",
       " \n",
       " It's softest brown, and brightest blue, \n",
       " \n",
       " The curved stitch here, reads `` made it May''. \n",
       " \n",
       " It's hard to see, but believe me it's true, \n",
       " \n",
       " That's not just a cloth but a piece of shirt. \n",
       " \n",
       " You can see a logo here, and right there, \n",
       " \n",
       " And a signature over there, someone named `` Bert''. \n",
       " \n",
       " This is my favorite part, a piece from a stuffed bear. \n",
       " \n",
       " I think it was my mother's favorite too, \n",
       " \n",
       " She always said so at least. \n",
       " \n",
       " Something from when she was two, \n",
       " \n",
       " Given by her grandad for Thanksgiving feast. \n",
       " \n",
       " My dad added this, a little button pin, \n",
       " \n",
       " Something from his mother, for being a scout. \n",
       " \n",
       " Apparently she went to a store and fished in a bin, \n",
       " \n",
       " Until night that day, to teach him what love was about. \n",
       " \n",
       " I'm sorry you had to see this, \n",
       " \n",
       " but their funeral was delayed. \n",
       " \n",
       " \n",
       " \n",
       "\n",
       "\n",
       "\n",
       " \n",
       "\n",
       "**<font color='blue'>Rewriten Text:</font>**\n",
       "The softest brown and brightest blue quilt, crafted by the loving hand of a mother, evokes vivid memories in my mind. The curved stitch, delicately etched upon its surface, bears the inscription \"made it May,\" a testament to the passage of time. Though the fabric may be veiled in mystery, I firmly believe that this quilt is not merely a cloth but a treasured heirloom, imbued with love and sentiment.\n",
       "\n",
       "The intricate logo and signature, meticulously woven into the quilt's fabric, reveal the name \"Bert,\" a symbol of the owner's identity. This precious artifact, once a part of a stuffed bear, holds a special place in my heart. It was my mother's favorite, a cherished relic from her youth, gifted to her by her grandfather during a Thanksgiving feast.\n",
       "\n",
       "My father added his own touch to the quilt, a pinned button that symbolized his mother's unwavering"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Take one sample\n",
    "row = df1_gem.iloc[1]\n",
    "\n",
    "# Generate Prompt using template\n",
    "prompt = gemini_prompts.format(\n",
    "    original_text=row.original_text,\n",
    "    rewritten_text=row.rewritten_text,\n",
    "    rewrite_prompt=\"\",\n",
    ")\n",
    "\n",
    "# Infer\n",
    "output = gemma_lm.generate(prompt, max_length=512)\n",
    "\n",
    "# Colorize\n",
    "output = colorize_text(output)\n",
    "\n",
    "# Display in markdown\n",
    "display(Markdown(output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6399daca",
   "metadata": {},
   "source": [
    "## Use of RF classifier to get the best prediction:\n",
    "- Using a RandomForest classifier\n",
    "    - The dataset is copied to mix the \"Rewrite prompt\" so that the output is wrong in order to create bad examples\n",
    "    - The correct examples then are assigned 1 and the incorrect 0\n",
    "    - The Classifier is trained to predict 0 or 1 using the entire dataset x2.\n",
    "    - This will be used accompanied with different generation result and the classifier select the best option, if theres more than one then select the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb54b54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    \n",
    "    # Join tokens back into text\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7badc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new df with good and wrong examples based on the dataset\n",
    "correct= df1_gem[[\"original_text\",\"rewritten_text\",\"rewrite_prompt\"]].copy()\n",
    "incorrect=df1_gem[[\"original_text\",\"rewritten_text\",\"rewrite_prompt\"]].copy()\n",
    "correct[\"target\"]=1\n",
    "incorrect[\"target\"]=0\n",
    "\n",
    "# Shuffle only column \n",
    "shuffled_column = incorrect['rewrite_prompt'].sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Create a new DataFrame with shuffled column  and other columns unchanged\n",
    "incorrect[\"rewrite_prompt\"] = shuffled_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86a4640",
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect.sample(frac=1, random_state=42)[:20000]\n",
    "#concat, shuffle and slice the datasets.\n",
    "rf_df= pd.concat([correct.sample(frac=1, random_state=42)[:40000], incorrect.sample(frac=1, random_state=42)[:40000]], ignore_index=True)\n",
    "#rf_df= rf_df.sample(frac=1, random_state=42).reset_index(inplace=True)\n",
    "\n",
    "rf_df=rf_df.sample(frac=1, random_state=42)\n",
    "rf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2709f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate text columns into a single column\n",
    "rf_df['combined_text'] = rf_df['original_text'] + ' ' + rf_df['rewritten_text'] + '<prompt>: ' + rf_df['rewrite_prompt']\n",
    "\n",
    "# Text Preprocessing (assuming you have a function called 'preprocess_text')\n",
    "rf_df['combined_text'] = rf_df['combined_text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0866c34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\"\"\"# Concatenate text columns into a single column\n",
    "df1_gem['combined_text'] = df1_gem['text_column1'] + ' ' + df1_gem['text_column2'] + ' ' + df1_gem['text_column3']\n",
    "\n",
    "# Text Preprocessing (assuming you have a function called 'preprocess_text')\n",
    "df1_gem['combined_text'] = df1_gem['combined_text'].apply(preprocess_text)\"\"\"\n",
    "\n",
    "# Text Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(rf_df['combined_text'])\n",
    "\n",
    "\n",
    "y_encoded = rf_df[\"target\"]\n",
    "\n",
    "# Data Splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model Training\n",
    "rf_classifier = RandomForestClassifier(n_estimators=300, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Model Evaluation\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7c0d6d",
   "metadata": {
    "papermill": {
     "duration": 0.860101,
     "end_time": "2024-04-15T11:00:15.698558",
     "exception": false,
     "start_time": "2024-04-15T11:00:14.838457",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "591e71bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-15T11:00:17.243021Z",
     "iopub.status.busy": "2024-04-15T11:00:17.242474Z",
     "iopub.status.idle": "2024-04-15T11:00:17.275971Z",
     "shell.execute_reply": "2024-04-15T11:00:17.275103Z"
    },
    "papermill": {
     "duration": 0.830029,
     "end_time": "2024-04-15T11:00:17.277876",
     "exception": false,
     "start_time": "2024-04-15T11:00:16.447847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original_text</th>\n",
       "      <th>rewritten_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>The competition dataset comprises text passage...</td>\n",
       "      <td>Here is your shanty: (Verse 1) The text is rew...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                      original_text  \\\n",
       "0  -1  The competition dataset comprises text passage...   \n",
       "\n",
       "                                      rewritten_text  \n",
       "0  Here is your shanty: (Verse 1) The text is rew...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"/kaggle/input/llm-prompt-recovery/test.csv\")\n",
    "test_df['original_text'] = test_df['original_text'].fillna(\"\")\n",
    "test_df['rewritten_text'] = test_df['rewritten_text'].fillna(\"\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdbab3d",
   "metadata": {
    "papermill": {
     "duration": 0.823336,
     "end_time": "2024-04-15T11:00:18.854649",
     "exception": false,
     "start_time": "2024-04-15T11:00:18.031313",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test Sample\n",
    "\n",
    "Now, let's try out a sample from test data that model hasn't seen during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbf9ccf7",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-04-15T11:00:20.420791Z",
     "iopub.status.busy": "2024-04-15T11:00:20.420409Z",
     "iopub.status.idle": "2024-04-15T11:00:20.967820Z",
     "shell.execute_reply": "2024-04-15T11:00:20.966831Z"
    },
    "papermill": {
     "duration": 1.317654,
     "end_time": "2024-04-15T11:00:20.969798",
     "exception": false,
     "start_time": "2024-04-15T11:00:19.652144",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<start_of_turn>user\n",
       "You are a reverse prompt engineer. Read the following \"Original Text\" and your task is to create the correct \"Instruction\" that instructs a LLM to generate the \"Rewriten Text\" accurately.\n",
       "\n",
       " \n",
       "\n",
       "**<font color='yellow'>Original Text:</font>**\n",
       "The competition dataset comprises text passages that have been rewritten by the Gemma LLM according to some rewrite_prompt instruction. The goal of the competition is to determine what prompt was used to rewrite each original text.  Please note that this is a Code Competition. When your submission is scored, this example test data will be replaced with the full test set. Expect roughly 2,000 original texts in the test set.\n",
       "\n",
       " \n",
       "\n",
       "**<font color='blue'>Rewriten Text:</font>**\n",
       "Here is your shanty: (Verse 1) The text is rewritten, the LLM has spun, With prompts so clever, they've been outrun. The goal is to find, the prompt so bright, To crack the code, and shine the light. (Chorus) Oh, this is a code competition, my dear, With text and prompts, we'll compete. Two thousand texts, a challenge grand, To guess the prompts, hand over hand.(Verse 2) The original text, a treasure lost, The rewrite prompt, a secret to be\n",
       "<end_of_turn>\n",
       "<start_of_turn>model\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Instruction:</font>**\n",
       "\n",
       "Rewrite the text as a shanty\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "row = test_df.iloc[0]\n",
    "\n",
    "# Generate Prompt using template\n",
    "prompt = gemini_prompts.format(\n",
    "    original_text=row.original_text,\n",
    "    rewritten_text=row.rewritten_text,\n",
    "    rewrite_prompt=\"\",\n",
    ")\n",
    "\n",
    "# Infer\n",
    "output = gemma_lm.generate(prompt, max_length=512)\n",
    "\n",
    "# Colorize\n",
    "output = colorize_text(output)\n",
    "\n",
    "# Display in markdown\n",
    "display(Markdown(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fab2aba",
   "metadata": {
    "papermill": {
     "duration": 0.812214,
     "end_time": "2024-04-15T11:00:22.530381",
     "exception": false,
     "start_time": "2024-04-15T11:00:21.718167",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da3bf337",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-15T11:00:24.022238Z",
     "iopub.status.busy": "2024-04-15T11:00:24.021880Z",
     "iopub.status.idle": "2024-04-15T11:00:24.583518Z",
     "shell.execute_reply": "2024-04-15T11:00:24.582555Z"
    },
    "papermill": {
     "duration": 1.306971,
     "end_time": "2024-04-15T11:00:24.585448",
     "exception": false,
     "start_time": "2024-04-15T11:00:23.278477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a9cd4e73f6943e482293b71befbc4f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = []\n",
    "for i in tqdm(range(len(test_df))):\n",
    "    row = test_df.iloc[i]\n",
    "\n",
    "    # Generate Prompt using template\n",
    "    prompt = gemini_prompts.format(\n",
    "        original_text=row.original_text,\n",
    "        rewritten_text=row.rewritten_text,\n",
    "        rewrite_prompt=\"\"\n",
    "    )\n",
    "\n",
    "    # Infer\n",
    "    output = gemma_lm.generate(prompt, max_length=512)\n",
    "    pred = output.replace(prompt, \"\") # remove the prompt from output\n",
    "    \n",
    "    # Store predictions\n",
    "    preds.append([row.id, pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dffbd3",
   "metadata": {
    "papermill": {
     "duration": 0.743393,
     "end_time": "2024-04-15T11:00:26.141514",
     "exception": false,
     "start_time": "2024-04-15T11:00:25.398121",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "While preparing the submission file, we must keep in mind that, leaving any `rewrite_prompt` blank as null answers will throw an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "66c3d539",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-15T11:00:27.652914Z",
     "iopub.status.busy": "2024-04-15T11:00:27.652457Z",
     "iopub.status.idle": "2024-04-15T11:00:27.667736Z",
     "shell.execute_reply": "2024-04-15T11:00:27.666861Z"
    },
    "papermill": {
     "duration": 0.78449,
     "end_time": "2024-04-15T11:00:27.669528",
     "exception": false,
     "start_time": "2024-04-15T11:00:26.885038",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>rewrite_prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>Rewrite this as a shanty.\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id               rewrite_prompt\n",
       "0  -1  Rewrite this as a shanty.\\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df = pd.DataFrame(preds, columns=[\"id\", \"rewrite_prompt\"])\n",
    "sub_df['rewrite_prompt'] = sub_df['rewrite_prompt'].fillna(\"\")\n",
    "sub_df['rewrite_prompt'] = sub_df['rewrite_prompt'].map(lambda x: \"Improve the essay\" if len(x) == 0 else x)\n",
    "sub_df.to_csv(\"submission.csv\",index=False)\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c197746",
   "metadata": {},
   "source": [
    "# Generation using the RF Classifier\n",
    "-Different samplers to obtain different results if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f93924a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial test\n",
    "#We need three different predictions from the trained Gemma model (3 could be changed)\n",
    "\n",
    "# Take one sample\n",
    "row = df1_gem.iloc[1]\n",
    "\n",
    "# Generate Prompt using template\n",
    "prompt = gemini_prompts.format(\n",
    "    original_text=row.original_text,\n",
    "    rewritten_text=row.rewritten_text,\n",
    "    rewrite_prompt=\"\",\n",
    ")\n",
    "\n",
    "# Infer with three diff samplers\n",
    "outputs=[]\n",
    "#for i in range(3):\n",
    "output = gemma_lm.generate(prompt, max_length=700)\n",
    "outputs.append(output.replace(prompt, \"\"))\n",
    "gemma_lm.compile(sampler=\"top_p\")\n",
    "output_p = gemma_lm.generate(prompt, max_length=700)\n",
    "outputs.append(output_p.replace(prompt, \"\"))\n",
    "\n",
    "gemma_lm.compile(sampler=\"top_k\")\n",
    "output_k = gemma_lm.generate(prompt, max_length=700)\n",
    "outputs.append(output_k.replace(prompt, \"\"))\n",
    "\n",
    "gemma_lm.compile(sampler=\"beam\")\n",
    "output_b = gemma_lm.generate(prompt, max_length=700)\n",
    "outputs.append(output_b.replace(prompt, \"\"))\n",
    "\n",
    "outputs_vec = tfidf_vectorizer.transform(outputs)\n",
    "preds_rf= rf_classifier.predict(outputs_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea966c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "row[\"rewrite_prompt\"]"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7806901,
     "sourceId": 67121,
     "sourceType": "competition"
    },
    {
     "datasetId": 4517764,
     "sourceId": 7731345,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4518936,
     "sourceId": 7733314,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4668661,
     "sourceId": 7940822,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 5388,
     "sourceId": 11372,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9505.356856,
   "end_time": "2024-04-15T11:00:35.381583",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-15T08:22:10.024727",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "07aeee8a2829468a8d96aa8b294877a9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8a59466c025b40c8acac77fd6ed58f3f",
       "placeholder": "​",
       "style": "IPY_MODEL_c2d41cb068914e34aa87eaa69bce8385",
       "value": "100%"
      }
     },
     "173013194e9e45ebb6dce0f5b253a2b1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "21512ed0f38b4b419ab897d389f337ba": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2a9cd4e73f6943e482293b71befbc4f5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_07aeee8a2829468a8d96aa8b294877a9",
        "IPY_MODEL_e746eae447db4ace8315667afda58705",
        "IPY_MODEL_b784b25779ce493cbc1368c09cbbff28"
       ],
       "layout": "IPY_MODEL_173013194e9e45ebb6dce0f5b253a2b1"
      }
     },
     "45c0987d34a240fcb18b61d438d75006": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "6e0d79bdd9314677afce4b912bb7a06b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "70842536c3fa4d0c8139d76ef9d79a0f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8a59466c025b40c8acac77fd6ed58f3f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b784b25779ce493cbc1368c09cbbff28": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_70842536c3fa4d0c8139d76ef9d79a0f",
       "placeholder": "​",
       "style": "IPY_MODEL_6e0d79bdd9314677afce4b912bb7a06b",
       "value": " 1/1 [00:00&lt;00:00,  1.82it/s]"
      }
     },
     "c2d41cb068914e34aa87eaa69bce8385": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "e746eae447db4ace8315667afda58705": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_21512ed0f38b4b419ab897d389f337ba",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_45c0987d34a240fcb18b61d438d75006",
       "value": 1
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
