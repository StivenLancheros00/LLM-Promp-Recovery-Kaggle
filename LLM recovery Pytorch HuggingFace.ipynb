{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ffae55a5d6ab413586503938ef2c2e36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b0b50e283cb4439ba804c12e8d13d662",
              "IPY_MODEL_f340993820374f1e99ed730ef8ae0c97",
              "IPY_MODEL_d3bf642556004895994ef2f193842440"
            ],
            "layout": "IPY_MODEL_9075e3d5f83c4f669fd44cc321bbea54"
          }
        },
        "b0b50e283cb4439ba804c12e8d13d662": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0bd258c862634bc88466f7caf94d7296",
            "placeholder": "​",
            "style": "IPY_MODEL_216800bd03784b61abade59f1736da61",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "f340993820374f1e99ed730ef8ae0c97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74036afb5a5d4a878dce363fc7943e36",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ae4e2b96cc4f458f9c8b75e3017c32b2",
            "value": 2
          }
        },
        "d3bf642556004895994ef2f193842440": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf4de6ac43b84b64acc20bd52d5ea8a2",
            "placeholder": "​",
            "style": "IPY_MODEL_c1a9e3b3d847427b9e8be0ee0b36a03f",
            "value": " 2/2 [00:31&lt;00:00, 13.01s/it]"
          }
        },
        "9075e3d5f83c4f669fd44cc321bbea54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bd258c862634bc88466f7caf94d7296": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "216800bd03784b61abade59f1736da61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74036afb5a5d4a878dce363fc7943e36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae4e2b96cc4f458f9c8b75e3017c32b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bf4de6ac43b84b64acc20bd52d5ea8a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1a9e3b3d847427b9e8be0ee0b36a03f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "abcf5217d2234f48864e23d215e73e16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_135b9ee2e0c94ac2a9e554e1fe8b5c48",
              "IPY_MODEL_836100ca461a450f8c35c1c5460b7045",
              "IPY_MODEL_9732ec2f70fb40ea90a9c11ab726a356"
            ],
            "layout": "IPY_MODEL_41916236f7a84216b26a56f17ec3ef6a"
          }
        },
        "135b9ee2e0c94ac2a9e554e1fe8b5c48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f334c9e38110425daa777c1898590b83",
            "placeholder": "​",
            "style": "IPY_MODEL_59e1c32b70874317bb45d14fa99f1033",
            "value": "Map: 100%"
          }
        },
        "836100ca461a450f8c35c1c5460b7045": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15c093250c094ac88c200ba5c9b1328b",
            "max": 3000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c2c811db381947c7b1a367c07d4befd0",
            "value": 3000
          }
        },
        "9732ec2f70fb40ea90a9c11ab726a356": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b94b95ce88349e7b4c4cddfa94d3cde",
            "placeholder": "​",
            "style": "IPY_MODEL_78356ce6ee7c40fda943868fad9f31fc",
            "value": " 3000/3000 [00:02&lt;00:00, 1023.96 examples/s]"
          }
        },
        "41916236f7a84216b26a56f17ec3ef6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f334c9e38110425daa777c1898590b83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59e1c32b70874317bb45d14fa99f1033": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "15c093250c094ac88c200ba5c9b1328b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2c811db381947c7b1a367c07d4befd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1b94b95ce88349e7b4c4cddfa94d3cde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78356ce6ee7c40fda943868fad9f31fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0eb33701c684480dbfef4e10e93669f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_db42d1ebd03743378b324ed5a6fd9331",
              "IPY_MODEL_a568927b607545d6811c5f5b9e293750",
              "IPY_MODEL_42c51d785dae435e9a9cbcb4ec03504c"
            ],
            "layout": "IPY_MODEL_8eb86828c6a54f609f4cdc873fd52676"
          }
        },
        "db42d1ebd03743378b324ed5a6fd9331": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf2921440bef47c59ff627af48bbb7f9",
            "placeholder": "​",
            "style": "IPY_MODEL_d9b1e32822ed4d518b626a3b3d8b4c3e",
            "value": "Map: 100%"
          }
        },
        "a568927b607545d6811c5f5b9e293750": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e02dbcfc109f4313bd8f8982886b5102",
            "max": 3000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dccd42f6432643eabf837b760ac597bb",
            "value": 3000
          }
        },
        "42c51d785dae435e9a9cbcb4ec03504c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_390a5bd7d5bb4bcb893ffd2b222fed00",
            "placeholder": "​",
            "style": "IPY_MODEL_80dd450796424e1ba7055104cc0a3082",
            "value": " 3000/3000 [00:04&lt;00:00, 727.58 examples/s]"
          }
        },
        "8eb86828c6a54f609f4cdc873fd52676": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf2921440bef47c59ff627af48bbb7f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9b1e32822ed4d518b626a3b3d8b4c3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e02dbcfc109f4313bd8f8982886b5102": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dccd42f6432643eabf837b760ac597bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "390a5bd7d5bb4bcb893ffd2b222fed00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80dd450796424e1ba7055104cc0a3082": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zRSGiFRGIRC"
      },
      "outputs": [],
      "source": [
        "pip install accelerate transformers evaluate datasets peft bitsandbytes loralib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kaggle competition\n",
        "## LLM prompt recovery"
      ],
      "metadata": {
        "id": "ez0Fn_zmxh-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#HuggingFace Login\n",
        "from huggingface_hub import interpreter_login\n",
        "interpreter_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CppewIpaHXqn",
        "outputId": "b08eeef9-65dc-4a39-94d1-18b21c859bb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: ··········\n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Import the necessary libraries and instantiate the instruction model\n",
        "-Loading in bit8 for memory reduction\n",
        "-Flash attention could be also used : attn_implementation=\"flash_attention_2\"\"\"\"\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import bitsandbytes as bnb\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"google/gemma-2b-it\",\n",
        "    load_in_8bit=True,\n",
        "    device_map='auto',\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190,
          "referenced_widgets": [
            "ffae55a5d6ab413586503938ef2c2e36",
            "b0b50e283cb4439ba804c12e8d13d662",
            "f340993820374f1e99ed730ef8ae0c97",
            "d3bf642556004895994ef2f193842440",
            "9075e3d5f83c4f669fd44cc321bbea54",
            "0bd258c862634bc88466f7caf94d7296",
            "216800bd03784b61abade59f1736da61",
            "74036afb5a5d4a878dce363fc7943e36",
            "ae4e2b96cc4f458f9c8b75e3017c32b2",
            "bf4de6ac43b84b64acc20bd52d5ea8a2",
            "c1a9e3b3d847427b9e8be0ee0b36a03f"
          ]
        },
        "id": "TcORXnXUG1c8",
        "outputId": "efb1bad9-e4b1-4221-d5c3-6f33658bb293"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ffae55a5d6ab413586503938ef2c2e36"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "  param.requires_grad = False  # freeze the model - train adapters later\n",
        "  if param.ndim == 1:\n",
        "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
        "    param.data = param.data.to(torch.float32)\n",
        "\n",
        "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "class CastOutputToFloat(nn.Sequential):\n",
        "  def forward(self, x): return super().forward(x).to(torch.float32)\n",
        "model.lm_head = CastOutputToFloat(model.lm_head)"
      ],
      "metadata": {
        "id": "D-D7a2Y0KDGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "cdL__202G91R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "#Target modules obtained from documentation, these modules are frozen and used for LoRa\n",
        "config = LoraConfig(\n",
        "    r=8, #attention heads\n",
        "    lora_alpha=32, #alpha scaling\n",
        "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], #if you know the\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\" # set this for CLM or Seq2Seq\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9IbE9F3KIxF",
        "outputId": "136080e5-344d-48c6-a396-d339ed83cf2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 9805824 || all params: 2515978240 || trainable%: 0.3897420034920493\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Installing Kaggle to obtain the dataset provided by the competition.\n",
        "#This test was run on Colab initially. If run in kaggle, then the following is not needed\n",
        "pip install kaggle"
      ],
      "metadata": {
        "id": "Zj_RnDlPMZAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuKl09YHNV3i",
        "outputId": "01f79749-fe2c-466a-a275-8a945a607d49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle."
      ],
      "metadata": {
        "id": "7_EQLFzzNq9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cp /kaggle/kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "gnXH_Oc4OOy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "5Ad2eemuOpkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download ibrahim2002/all-in-one-dataset-with-embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwect7fbMiXm",
        "outputId": "14521fb3-6020-42f1-a89f-49fc6f0929bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading all-in-one-dataset-with-embedding.zip to /content\n",
            " 99% 1.24G/1.26G [00:18<00:00, 49.0MB/s]\n",
            "100% 1.26G/1.26G [00:19<00:00, 70.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip /content/all-in-one-dataset-with-embedding.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAezvMh9O28m",
        "outputId": "3a9d34fc-3acb-47d2-bc49-f5c55a721190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/all-in-one-dataset-with-embedding.zip\n",
            "  inflating: df_with_emb.csv         \n",
            "  inflating: df_with_emb.parquet     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df= pd.read_csv(\"/content/df_with_emb.csv\")"
      ],
      "metadata": {
        "id": "BC40PuxPPIBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "5oRte4IpPM5T",
        "outputId": "18483fbc-238f-45ff-bb91-27344e82f9c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                       original_text  \\\n",
              "0  Dear Randy,\\n\\nI hope this letter finds you we...   \n",
              "1  This quilt, that my mother made, \\n \\n Still m...   \n",
              "2  It's the job of our agency to keep track of th...   \n",
              "3  The first punch gets me right in the ribs, kno...   \n",
              "4  Some nights I lay awake staring at the ceiling...   \n",
              "\n",
              "                                      rewrite_prompt  \\\n",
              "0  Rephrase this letter to infuse it with an elfi...   \n",
              "1  Regency Romance: Model the text on a Regency r...   \n",
              "2  Write like Ernest Hemingway: Focus on Hemingwa...   \n",
              "3  Grimm's Fairy Tales: Adapt the text to mimic t...   \n",
              "4  High Fantasy Epic: Transform the essay into a ...   \n",
              "\n",
              "                                      rewritten_text dataset_id  \\\n",
              "0  Dear Randy,\\n\\nMay this enchanted message find...       host   \n",
              "1  The softest brown and brightest blue quilt, cr...   nbroad_1   \n",
              "2  The agency's responsibility is to track and co...   nbroad_1   \n",
              "3  In the sweltering sun, the stench of sweat and...   nbroad_1   \n",
              "4  In the tapestry of the ethereal realm of Eldri...   nbroad_1   \n",
              "\n",
              "   original_text_emb_0  original_text_emb_1  original_text_emb_2  \\\n",
              "0            -0.028678            -0.072260            -0.003482   \n",
              "1            -0.007521            -0.082697             0.041606   \n",
              "2            -0.012435            -0.057448             0.041290   \n",
              "3             0.023922            -0.051777             0.037086   \n",
              "4            -0.002841            -0.056669             0.049056   \n",
              "\n",
              "   original_text_emb_3  original_text_emb_4  original_text_emb_5  ...  \\\n",
              "0             0.050595            -0.010600            -0.023786  ...   \n",
              "1             0.048020             0.007870            -0.018765  ...   \n",
              "2             0.021997            -0.005588            -0.011431  ...   \n",
              "3             0.057018             0.006423             0.006742  ...   \n",
              "4             0.072066            -0.006430            -0.020034  ...   \n",
              "\n",
              "   rewritten_text_emb_758  rewritten_text_emb_759  rewritten_text_emb_760  \\\n",
              "0               -0.023307                0.011079               -0.026521   \n",
              "1               -0.046357               -0.019648               -0.044074   \n",
              "2               -0.009061               -0.024727               -0.005645   \n",
              "3               -0.024975                0.006210               -0.010690   \n",
              "4               -0.031516                0.014793               -0.024928   \n",
              "\n",
              "   rewritten_text_emb_761  rewritten_text_emb_762  rewritten_text_emb_763  \\\n",
              "0               -0.035838               -0.011306                0.035690   \n",
              "1               -0.049267                0.000438                0.047913   \n",
              "2               -0.049009               -0.007487                0.029255   \n",
              "3               -0.057320                0.015437                0.041056   \n",
              "4               -0.052689               -0.005655                0.044724   \n",
              "\n",
              "   rewritten_text_emb_764  rewritten_text_emb_765  rewritten_text_emb_766  \\\n",
              "0               -0.005810                0.004506               -0.037956   \n",
              "1                0.042219                0.012958               -0.048047   \n",
              "2                0.023511                0.006769               -0.041317   \n",
              "3                0.012486               -0.006540               -0.030836   \n",
              "4                0.016817               -0.001000               -0.026472   \n",
              "\n",
              "   rewritten_text_emb_767  \n",
              "0                0.001139  \n",
              "1                0.006332  \n",
              "2               -0.012102  \n",
              "3               -0.000440  \n",
              "4                0.008088  \n",
              "\n",
              "[5 rows x 2308 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-253130a0-cf2d-4c53-a411-1e5ea1c44d21\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original_text</th>\n",
              "      <th>rewrite_prompt</th>\n",
              "      <th>rewritten_text</th>\n",
              "      <th>dataset_id</th>\n",
              "      <th>original_text_emb_0</th>\n",
              "      <th>original_text_emb_1</th>\n",
              "      <th>original_text_emb_2</th>\n",
              "      <th>original_text_emb_3</th>\n",
              "      <th>original_text_emb_4</th>\n",
              "      <th>original_text_emb_5</th>\n",
              "      <th>...</th>\n",
              "      <th>rewritten_text_emb_758</th>\n",
              "      <th>rewritten_text_emb_759</th>\n",
              "      <th>rewritten_text_emb_760</th>\n",
              "      <th>rewritten_text_emb_761</th>\n",
              "      <th>rewritten_text_emb_762</th>\n",
              "      <th>rewritten_text_emb_763</th>\n",
              "      <th>rewritten_text_emb_764</th>\n",
              "      <th>rewritten_text_emb_765</th>\n",
              "      <th>rewritten_text_emb_766</th>\n",
              "      <th>rewritten_text_emb_767</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Dear Randy,\\n\\nI hope this letter finds you we...</td>\n",
              "      <td>Rephrase this letter to infuse it with an elfi...</td>\n",
              "      <td>Dear Randy,\\n\\nMay this enchanted message find...</td>\n",
              "      <td>host</td>\n",
              "      <td>-0.028678</td>\n",
              "      <td>-0.072260</td>\n",
              "      <td>-0.003482</td>\n",
              "      <td>0.050595</td>\n",
              "      <td>-0.010600</td>\n",
              "      <td>-0.023786</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.023307</td>\n",
              "      <td>0.011079</td>\n",
              "      <td>-0.026521</td>\n",
              "      <td>-0.035838</td>\n",
              "      <td>-0.011306</td>\n",
              "      <td>0.035690</td>\n",
              "      <td>-0.005810</td>\n",
              "      <td>0.004506</td>\n",
              "      <td>-0.037956</td>\n",
              "      <td>0.001139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This quilt, that my mother made, \\n \\n Still m...</td>\n",
              "      <td>Regency Romance: Model the text on a Regency r...</td>\n",
              "      <td>The softest brown and brightest blue quilt, cr...</td>\n",
              "      <td>nbroad_1</td>\n",
              "      <td>-0.007521</td>\n",
              "      <td>-0.082697</td>\n",
              "      <td>0.041606</td>\n",
              "      <td>0.048020</td>\n",
              "      <td>0.007870</td>\n",
              "      <td>-0.018765</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.046357</td>\n",
              "      <td>-0.019648</td>\n",
              "      <td>-0.044074</td>\n",
              "      <td>-0.049267</td>\n",
              "      <td>0.000438</td>\n",
              "      <td>0.047913</td>\n",
              "      <td>0.042219</td>\n",
              "      <td>0.012958</td>\n",
              "      <td>-0.048047</td>\n",
              "      <td>0.006332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>It's the job of our agency to keep track of th...</td>\n",
              "      <td>Write like Ernest Hemingway: Focus on Hemingwa...</td>\n",
              "      <td>The agency's responsibility is to track and co...</td>\n",
              "      <td>nbroad_1</td>\n",
              "      <td>-0.012435</td>\n",
              "      <td>-0.057448</td>\n",
              "      <td>0.041290</td>\n",
              "      <td>0.021997</td>\n",
              "      <td>-0.005588</td>\n",
              "      <td>-0.011431</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.009061</td>\n",
              "      <td>-0.024727</td>\n",
              "      <td>-0.005645</td>\n",
              "      <td>-0.049009</td>\n",
              "      <td>-0.007487</td>\n",
              "      <td>0.029255</td>\n",
              "      <td>0.023511</td>\n",
              "      <td>0.006769</td>\n",
              "      <td>-0.041317</td>\n",
              "      <td>-0.012102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The first punch gets me right in the ribs, kno...</td>\n",
              "      <td>Grimm's Fairy Tales: Adapt the text to mimic t...</td>\n",
              "      <td>In the sweltering sun, the stench of sweat and...</td>\n",
              "      <td>nbroad_1</td>\n",
              "      <td>0.023922</td>\n",
              "      <td>-0.051777</td>\n",
              "      <td>0.037086</td>\n",
              "      <td>0.057018</td>\n",
              "      <td>0.006423</td>\n",
              "      <td>0.006742</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.024975</td>\n",
              "      <td>0.006210</td>\n",
              "      <td>-0.010690</td>\n",
              "      <td>-0.057320</td>\n",
              "      <td>0.015437</td>\n",
              "      <td>0.041056</td>\n",
              "      <td>0.012486</td>\n",
              "      <td>-0.006540</td>\n",
              "      <td>-0.030836</td>\n",
              "      <td>-0.000440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Some nights I lay awake staring at the ceiling...</td>\n",
              "      <td>High Fantasy Epic: Transform the essay into a ...</td>\n",
              "      <td>In the tapestry of the ethereal realm of Eldri...</td>\n",
              "      <td>nbroad_1</td>\n",
              "      <td>-0.002841</td>\n",
              "      <td>-0.056669</td>\n",
              "      <td>0.049056</td>\n",
              "      <td>0.072066</td>\n",
              "      <td>-0.006430</td>\n",
              "      <td>-0.020034</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.031516</td>\n",
              "      <td>0.014793</td>\n",
              "      <td>-0.024928</td>\n",
              "      <td>-0.052689</td>\n",
              "      <td>-0.005655</td>\n",
              "      <td>0.044724</td>\n",
              "      <td>0.016817</td>\n",
              "      <td>-0.001000</td>\n",
              "      <td>-0.026472</td>\n",
              "      <td>0.008088</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 2308 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-253130a0-cf2d-4c53-a411-1e5ea1c44d21')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-253130a0-cf2d-4c53-a411-1e5ea1c44d21 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-253130a0-cf2d-4c53-a411-1e5ea1c44d21');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-40c0697d-eebb-48b0-a0e7-65faa6a16f84\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-40c0697d-eebb-48b0-a0e7-65faa6a16f84')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-40c0697d-eebb-48b0-a0e7-65faa6a16f84 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Formatting the dataset\n",
        "- Using only the needed columns.\n",
        "- The dataset was provided with embeddings. I decided to tokenize and vectorize using the model provided in the huggingface platform. Therefore, embeddings are ignored.\n",
        "- To avoid memory error while using Kaggle free tier or Colab free tier. The tokens were counted and selected no more than 600 or 700."
      ],
      "metadata": {
        "id": "OpjsaVtyySf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1= df[[\"dataset_id\", \"original_text\", \"rewrite_prompt\",\"rewritten_text\"]]\n"
      ],
      "metadata": {
        "id": "ApvT4UcTRXHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['token_count'] = df1[\"original_text\"].str.replace(',','').str.split().str.len()\n",
        "df1['token_count_rewrit'] = df1[\"rewritten_text\"].str.replace(',','').str.split().str.len()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qxu__fjGPt7k",
        "outputId": "71a14ae2-104e-4bec-d950-5ed9e74609e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-3313dc9a2b63>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df1['token_count'] = df1[\"original_text\"].str.replace(',','').str.split().str.len()\n",
            "<ipython-input-13-3313dc9a2b63>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df1['token_count_rewrit'] = df1[\"rewritten_text\"].str.replace(',','').str.split().str.len()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VM0xPsl7Qhw1",
        "outputId": "e3143f4c-d40b-46c5-b77e-98fb9051cbdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(58174, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2= df1[(df1[\"token_count\"]<600)&(df1[\"token_count_rewrit\"]<600)]"
      ],
      "metadata": {
        "id": "gA--U_BmQWat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The prompts for training will follow the reported template on the Gemini Paper. To leverage transfer learning and not changing the initial format.\n",
        "\"\"\"\n",
        "\n",
        "gemini_prompts= \"\"\"\n",
        "<start_of_turn>user\n",
        "You are a reverse prompt engineer. Read the following \"Original Text\" and create the correct \"Instruction\" that instructs a LLM to generate the the \"Rewriten Text\" accurately.\n",
        "\\n Original Text:\\n{original_text}\n",
        "\\n Rewriten Text:\\n{rewritten_text}\n",
        "<end_of_turn>\n",
        "<start_of_turn>model\n",
        "\\nInstruction:\\n{rewrite_prompt}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "kfdBmAHFPca0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2[\"prompt\"]= df2.apply(lambda row: gemini_prompts.format(original_text=row.original_text,\n",
        "                                                             rewritten_text=row.rewritten_text,\n",
        "                                                             rewrite_prompt=row.rewrite_prompt), axis=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpp3zEJkPmFk",
        "outputId": "f69649ef-720c-4559-93a2-610c9a48a1cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-e54883f677b9>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df2[\"prompt\"]= df2.apply(lambda row: gemini_prompts.format(original_text=row.original_text,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 753
        },
        "id": "_PKMTENBReRO",
        "outputId": "d473065c-477a-4618-d93d-466b30b76dbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      dataset_id                                      original_text  \\\n",
              "0           host  Dear Randy,\\n\\nI hope this letter finds you we...   \n",
              "1       nbroad_1  This quilt, that my mother made, \\n \\n Still m...   \n",
              "3       nbroad_1  The first punch gets me right in the ribs, kno...   \n",
              "5       nbroad_1  I can hardly read the letter, because the hand...   \n",
              "8       nbroad_1  `` Brigands and cutpurses have nothing on me. ...   \n",
              "...          ...                                                ...   \n",
              "58169  aatiffraz  I knew I only had 10 seconds to change things....   \n",
              "58170  aatiffraz  The rebels ran into the building lining up in ...   \n",
              "58171  aatiffraz  Jeanne loomed over the coffin, a great nagging...   \n",
              "58172  aatiffraz  The officers examined the graphic scene. The v...   \n",
              "58173  aatiffraz  Stacey ambulated across the bedroom towards me...   \n",
              "\n",
              "                                          rewrite_prompt  \\\n",
              "0      Rephrase this letter to infuse it with an elfi...   \n",
              "1      Regency Romance: Model the text on a Regency r...   \n",
              "3      Grimm's Fairy Tales: Adapt the text to mimic t...   \n",
              "5      Fairy Tale Villain: Use the menacing and craft...   \n",
              "8      Drunkard: Infuse the essay with the rambling, ...   \n",
              "...                                                  ...   \n",
              "58169  Rewrite this text in the style of a philosophi...   \n",
              "58170  Restyle this text as if it were written by a p...   \n",
              "58171  Restyle this text as if it were written by a p...   \n",
              "58172  Translate the essence of this text into a pira...   \n",
              "58173  Convey the same message as this text but throu...   \n",
              "\n",
              "                                          rewritten_text  token_count  \\\n",
              "0      Dear Randy,\\n\\nMay this enchanted message find...           63   \n",
              "1      The softest brown and brightest blue quilt, cr...          153   \n",
              "3      In the sweltering sun, the stench of sweat and...          284   \n",
              "5      My hand quivered as I clutched the letter, the...          463   \n",
              "8      \"Swerry brutes and cutthroat cutpurse, they ai...          260   \n",
              "...                                                  ...          ...   \n",
              "58169  \\n\\n## The Knight's Tale of Temporal Flux:\\n\\n...          138   \n",
              "58170  \\n\\nSure, here is the text rewritten as if it ...          108   \n",
              "58171  \\n\\nSure, here is the text rewritten as if it ...          147   \n",
              "58172  \\n\\n**Pirate Narrative:**\\n\\nAvast, me heartie...          102   \n",
              "58173  \\n\\nI have rewritten this text so that the mes...          135   \n",
              "\n",
              "       token_count_rewrit                                             prompt  \n",
              "0                      67  \\n<start_of_turn>user\\nYou are a reverse promp...  \n",
              "1                     203  \\n<start_of_turn>user\\nYou are a reverse promp...  \n",
              "3                     258  \\n<start_of_turn>user\\nYou are a reverse promp...  \n",
              "5                     272  \\n<start_of_turn>user\\nYou are a reverse promp...  \n",
              "8                     172  \\n<start_of_turn>user\\nYou are a reverse promp...  \n",
              "...                   ...                                                ...  \n",
              "58169                 153  \\n<start_of_turn>user\\nYou are a reverse promp...  \n",
              "58170                 160  \\n<start_of_turn>user\\nYou are a reverse promp...  \n",
              "58171                 151  \\n<start_of_turn>user\\nYou are a reverse promp...  \n",
              "58172                 118  \\n<start_of_turn>user\\nYou are a reverse promp...  \n",
              "58173                 140  \\n<start_of_turn>user\\nYou are a reverse promp...  \n",
              "\n",
              "[55902 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-15390ab2-b113-4175-8dd1-4fa2010ffb6a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dataset_id</th>\n",
              "      <th>original_text</th>\n",
              "      <th>rewrite_prompt</th>\n",
              "      <th>rewritten_text</th>\n",
              "      <th>token_count</th>\n",
              "      <th>token_count_rewrit</th>\n",
              "      <th>prompt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>host</td>\n",
              "      <td>Dear Randy,\\n\\nI hope this letter finds you we...</td>\n",
              "      <td>Rephrase this letter to infuse it with an elfi...</td>\n",
              "      <td>Dear Randy,\\n\\nMay this enchanted message find...</td>\n",
              "      <td>63</td>\n",
              "      <td>67</td>\n",
              "      <td>\\n&lt;start_of_turn&gt;user\\nYou are a reverse promp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>nbroad_1</td>\n",
              "      <td>This quilt, that my mother made, \\n \\n Still m...</td>\n",
              "      <td>Regency Romance: Model the text on a Regency r...</td>\n",
              "      <td>The softest brown and brightest blue quilt, cr...</td>\n",
              "      <td>153</td>\n",
              "      <td>203</td>\n",
              "      <td>\\n&lt;start_of_turn&gt;user\\nYou are a reverse promp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>nbroad_1</td>\n",
              "      <td>The first punch gets me right in the ribs, kno...</td>\n",
              "      <td>Grimm's Fairy Tales: Adapt the text to mimic t...</td>\n",
              "      <td>In the sweltering sun, the stench of sweat and...</td>\n",
              "      <td>284</td>\n",
              "      <td>258</td>\n",
              "      <td>\\n&lt;start_of_turn&gt;user\\nYou are a reverse promp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>nbroad_1</td>\n",
              "      <td>I can hardly read the letter, because the hand...</td>\n",
              "      <td>Fairy Tale Villain: Use the menacing and craft...</td>\n",
              "      <td>My hand quivered as I clutched the letter, the...</td>\n",
              "      <td>463</td>\n",
              "      <td>272</td>\n",
              "      <td>\\n&lt;start_of_turn&gt;user\\nYou are a reverse promp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>nbroad_1</td>\n",
              "      <td>`` Brigands and cutpurses have nothing on me. ...</td>\n",
              "      <td>Drunkard: Infuse the essay with the rambling, ...</td>\n",
              "      <td>\"Swerry brutes and cutthroat cutpurse, they ai...</td>\n",
              "      <td>260</td>\n",
              "      <td>172</td>\n",
              "      <td>\\n&lt;start_of_turn&gt;user\\nYou are a reverse promp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58169</th>\n",
              "      <td>aatiffraz</td>\n",
              "      <td>I knew I only had 10 seconds to change things....</td>\n",
              "      <td>Rewrite this text in the style of a philosophi...</td>\n",
              "      <td>\\n\\n## The Knight's Tale of Temporal Flux:\\n\\n...</td>\n",
              "      <td>138</td>\n",
              "      <td>153</td>\n",
              "      <td>\\n&lt;start_of_turn&gt;user\\nYou are a reverse promp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58170</th>\n",
              "      <td>aatiffraz</td>\n",
              "      <td>The rebels ran into the building lining up in ...</td>\n",
              "      <td>Restyle this text as if it were written by a p...</td>\n",
              "      <td>\\n\\nSure, here is the text rewritten as if it ...</td>\n",
              "      <td>108</td>\n",
              "      <td>160</td>\n",
              "      <td>\\n&lt;start_of_turn&gt;user\\nYou are a reverse promp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58171</th>\n",
              "      <td>aatiffraz</td>\n",
              "      <td>Jeanne loomed over the coffin, a great nagging...</td>\n",
              "      <td>Restyle this text as if it were written by a p...</td>\n",
              "      <td>\\n\\nSure, here is the text rewritten as if it ...</td>\n",
              "      <td>147</td>\n",
              "      <td>151</td>\n",
              "      <td>\\n&lt;start_of_turn&gt;user\\nYou are a reverse promp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58172</th>\n",
              "      <td>aatiffraz</td>\n",
              "      <td>The officers examined the graphic scene. The v...</td>\n",
              "      <td>Translate the essence of this text into a pira...</td>\n",
              "      <td>\\n\\n**Pirate Narrative:**\\n\\nAvast, me heartie...</td>\n",
              "      <td>102</td>\n",
              "      <td>118</td>\n",
              "      <td>\\n&lt;start_of_turn&gt;user\\nYou are a reverse promp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58173</th>\n",
              "      <td>aatiffraz</td>\n",
              "      <td>Stacey ambulated across the bedroom towards me...</td>\n",
              "      <td>Convey the same message as this text but throu...</td>\n",
              "      <td>\\n\\nI have rewritten this text so that the mes...</td>\n",
              "      <td>135</td>\n",
              "      <td>140</td>\n",
              "      <td>\\n&lt;start_of_turn&gt;user\\nYou are a reverse promp...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>55902 rows × 7 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-15390ab2-b113-4175-8dd1-4fa2010ffb6a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-15390ab2-b113-4175-8dd1-4fa2010ffb6a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-15390ab2-b113-4175-8dd1-4fa2010ffb6a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-90e5bc28-9da3-4da8-a0ba-7e2852f761be\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-90e5bc28-9da3-4da8-a0ba-7e2852f761be')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-90e5bc28-9da3-4da8-a0ba-7e2852f761be button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df2",
              "summary": "{\n  \"name\": \"df2\",\n  \"rows\": 55902,\n  \"fields\": [\n    {\n      \"column\": \"dataset_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 19,\n        \"samples\": [\n          \"host\",\n          \"thedrcat_3\",\n          \"isakatsuyoshi_1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"original_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 53421,\n        \"samples\": [\n          \"The other kids would always look at me differently, but in the mirror i could never See anything different. I lived it a world where I could n't understand many people, my parents were some of the only people that Spoke my language. \\n \\n No one wanted to play with me at recess. I'd look around at the faces of the other children, they would be laughing at me, but I never knew why. Until I met a girl, she knew my language, she explained they were making fun of the fact I could n't hear them, so they could say whatever they wanted. She gestured to her ears, then told me I was n't alone. \\n \\n \\n\\n\",\n          \"The process was quite simple really. All the angels across the world gathered together in one place ( usually a pub ) to vote on who next would be God amongst the mortals of earth. Only a simple majority was needed. \\n \\n To kick the election off every angel casted a blind vote. They picked any human on earth in an effort to try and work out who the early favourite were amongst their colleagues. \\n \\n So tactical angels casted their vote for the most incompetent, laziest, ugliest, most pitiful human they could think of who no one else would ever vote for. \\n \\n Funnily enough, they were all thinking of the same person. \\n \\n Me. \\n\\n\",\n          \"49ers receiver Jeremy Kerley steps into the right slot Video: Inform video embed Bold predictions: 49ers at Panthers Kristal Kerley\\u2019s hands were shaking before kickoff Monday at Levi\\u2019s Stadium. Her husband, wide receiver Jeremy Kerley, was about to make his 49ers debut, 15 days after he\\u2019d arrived in a trade. It was a prime opportunity to revive a solid career that had gone south. After 182 catches in five seasons with the Jets, he was released in March. After five months in Detroit, he was dealt to the 49ers for a backup guard, Brandon Thomas, who is on the Lions\\u2019\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rewrite_prompt\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7206,\n        \"samples\": [\n          \"Restyle this text as if it were written by a futuristic AI from romance novel.\",\n          \"Rewrite the essay entirely in alliteration\",\n          \"Make it a detailed report by a private investigator.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rewritten_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 55843,\n        \"samples\": [\n          \"## Meditation App Session Script\\n\\n**Welcome:**\\n\\n(Soft, calming music begins)\\n\\n**Greetings:**\\n\\n(Voice, like a gentle breeze)\\n\\nHello, and welcome to this space of tranquility. Please take a few comfortable breaths, closing your eyes and letting go of any distractions.\\n\\n**Grounding:**\\n\\n(A sense of sinking into a comfortable chair)\\n\\nFeel your feet touching the ground, the seat beneath you, and the back of your chair supporting you. Let your thoughts drift towards the present moment, noticing the sensation of your breathing and the surroundings.\\n\\n**Mindful Awareness:**\\n\\n(Quiet, introductory music)\\n\\nNow, open your senses to the world around you. Notice the smells, sounds, sights, tastes, and textures you perceive. Bring your awareness to your body, focusing on the sensations of your breath, your muscles, and your thoughts.\\n\\n**Cultivating Inner Peace:**\\n\\n(Soothing music fades in)\\n\\nAllow your mind to wander freely, explore your thoughts and emotions without judgment. Let go of worries and anxieties, and simply be present in this moment.\\n\\n**Compassion:**\\n\\n(Soft, meditative music)\\n\\nAs you sit in silence, extend your compassion to yourself and others. Imagine yourself radiating kindness, understanding,\",\n          \"**Alien Visitor:** Greetings, Earthling! I am from a distant planet where salt is unknown. Explain this strange substance to me.\\n\\n**Earthling:** Salt is a white crystal that is found on our planet. It is used to add flavor to food and drink. It makes their flavors stronger and more enjoyable.\\n\\n**Alien Visitor:** How does salt work?\\n\\n**Earthling:** Salt has tiny particles that interact with the taste buds on your tongue. These particles send signals to your brain\",\n          \"## Conference Proposal - \\\"Driving Safely: Addressing Unsafe Practices on the Road\\\"\\n\\n**Date:** [Date]\\n**Location:** [Location]\\n**Time:** [Time]\\n**Duration:** [Duration]\\n**Participants:** Law enforcement officials, traffic safety experts, community leaders, and concerned citizens.\\n\\n**Objectives:**\\n\\n* To raise awareness about unsafe driving practices, particularly the issue of attaching cargo to vehicles insecurely.\\n* To educate drivers on the dangers of improper cargo attachment and the potential consequences.\\n* To promote the importance of safety regulations and their compliance.\\n* To discuss solutions to address this issue and create a safer road environment.\\n\\n**Agenda:**\\n\\n* **Welcome and Introduction:** [Chair person]\\n* **Overview of the Problem:** Traffic safety experts presenting statistics and data on cargo-related accidents.\\n* **Case Studies:** Presentation of real-world examples of unsafe cargo attachment and the resulting consequences.\\n* **Impact on Victims:** Stories of victims and families affected by unsecured cargo accidents.\\n* **Discussion and Solutions:** Open dialogue on potential solutions to address the issue, including:\\n    * Enactment and enforcement of stricter regulations.\\n    * Educational campaigns for drivers and cargo loaders on safe cargo attachment techniques. \\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"token_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 94,\n        \"min\": 2,\n        \"max\": 599,\n        \"num_unique_values\": 598,\n        \"samples\": [\n          401,\n          194,\n          13\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"token_count_rewrit\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 79,\n        \"min\": 1,\n        \"max\": 575,\n        \"num_unique_values\": 510,\n        \"samples\": [\n          61,\n          62,\n          140\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prompt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 55902,\n        \"samples\": [\n          \"\\n<start_of_turn>user\\nYou are a reverse prompt engineer. Read the following \\\"Original Text\\\" and create the correct \\\"Instruction\\\" that instructs a LLM to generate the the \\\"Rewriten Text\\\" accurately.\\n\\n Original Text:\\nEmergency services were called to the scene at 08:39 BST on Tuesday, near the Siabod Cafe on the main A5 road in Conwy county.\\nThe crash involved a red Seat Leon and a silver Mercedes car, and the driver of the Seat vehicles was pronounced dead at the scene of the incident.\\nThe road between Capel Curig and Betws-y-Coed has now reopened.\\nNorth Wales Police said no-one else was seriously injured in the crash, but they have appealed for any witnesses to come forward.\\n\\n Rewriten Text:\\nDear Present Self,\\n\\nI know I'm writing to you from a future beyond your comprehension, but I couldn't bear the thought of you experiencing the tragedy that unfolded on the A5 road in Conwy county.\\n\\nIt was a cold evening, and the road was wet, but that shouldn't have been an excuse for driving so dangerously. You must have been devastated by the loss of your beloved Seat Leon. I know how much you loved that car, and it's a cruel irony that it was taken away from you on that night.\\n\\nI understand that you're probably feeling grief, anger, and disbelief right now. I wish I could be there to comfort you in this difficult time. If only I could turn back time and warn you about what was going to happen, I would. But I know that's not possible.\\n\\nSo, I'm writing to you with the hope that you will remember this day and take a lesson from it. Whenever you get behind the wheel, please drive responsibly. Slow down, be aware of your surroundings, and avoid texting or using your phone while driving.\\n\\nI know you're a strong person and that you will overcome this tragedy. I also know that you will never forget this day\\n<end_of_turn>\\n<start_of_turn>model\\n\\nInstruction:\\nWrite it as a heartfelt plea from the future to the present.\\n\",\n          \"\\n<start_of_turn>user\\nYou are a reverse prompt engineer. Read the following \\\"Original Text\\\" and create the correct \\\"Instruction\\\" that instructs a LLM to generate the the \\\"Rewriten Text\\\" accurately.\\n\\n Original Text:\\n- Reduce speed and absorb kinetic energy on reentry.\\n- Use heat shields\\n- Use parachutes\\n\\n Rewriten Text:\\n\\ud83d\\ude80 Spacecraft Reentry Tips! \\ud83d\\ude80\\n\\nTo safely return to Earth from space, follow these essential steps:\\n\\n\\u2705 Slow Down: Reduce speed to absorb kinetic energy during reentry.\\n\\u2705 Heat Protection: Shield your spacecraft with heat-resistant materials to withstand extreme temperatures.\\n\\u2705 Parachute Power: Deploy parachutes to slow you down further and ensure a gentle landing.\\n\\nLet's conquer the cosmos safely! \\ud83d\\ude80\\ud83c\\udf0d #SpaceTravel #ReentryEssentials\\n<end_of_turn>\\n<start_of_turn>model\\n\\nInstruction:\\nExpress this as if it were a social media post.\\n\",\n          \"\\n<start_of_turn>user\\nYou are a reverse prompt engineer. Read the following \\\"Original Text\\\" and create the correct \\\"Instruction\\\" that instructs a LLM to generate the the \\\"Rewriten Text\\\" accurately.\\n\\n Original Text:\\nThere was a knock at Professor Malcap \\u2019 s door that startled him awake. He didn \\u2019 t remember that he had napped at the office, and was briefly confused as he awoke. \\n \\n \\u201c It \\u2019 s not office hours. \\u201d He yelled to the door. The knocking came again. \\u201c Come back tomorrow between 4 and 4:30. \\u201d \\n \\n The knocking wouldn \\u2019 t stop. The Professor moved towards the door, and as he moved, he thought he could hear music coming from the other side. Or, what would pass as music for the college students he taught. It \\u2019 s all rubbish now, not like when he was young and spry. \\n \\n He opened the door and was immediately struck in the face by a six foot tall letter. As he hit the floor, he saw a gun shoved in his face. He closed his eyes and the letter began to speak. \\n \\n **'' X gon give it to ya'' **\\n\\n\\n Rewriten Text:\\nThere was a knock at Professor Malcap \\u2019 s door that startled him awake. He didn \\u2019 t remember that he had napped at the office, and was briefly confused as he awoke. \\n\\n\\u201c It \\u2019s not office hours. \\u201d He yelled to the door. The knocking came again. \\u201c Come back tomorrow between 4 and 4:30. \\u201d\\n\\nThe knocking wouldn \\u2019t stop. The Professor moved towards the door, and as he moved, he thought he could hear music coming from the other side. Or, what would pass as music for the college students he taught. It \\u2019s all rubbish now, not like when he was young and spry.\\n\\nHe opened the door and was immediately struck in the face by a six-foot-tall letter. As he hit the floor, he saw a gun shoved in his face. He closed his eyes and the letter began to speak.\\n\\n**'' X, you gon give it to ya. You are a slave, and I am your master. ''**\\n<end_of_turn>\\n<start_of_turn>model\\n\\nInstruction:\\nRewrite the essay with a strong theme of freedom, and X being a slave (either literally or metaphorically)\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2['token_count_promp'] = df2[\"prompt\"].str.replace(',','').str.split().str.len()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKkCM9dPRiQ8",
        "outputId": "654d6003-72a7-4412-c7ac-8d8695d1afa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-b5a124a41fb7>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df2['token_count_promp'] = df2[\"prompt\"].str.replace(',','').str.split().str.len()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = df2[df2[\"token_count_promp\"]<600]"
      ],
      "metadata": {
        "id": "wz2IHFHuRqmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_df = final_df.sample(3000)"
      ],
      "metadata": {
        "id": "KZY_RAilSMVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a dataset from the pandas dataframe for easier training\n",
        "\n",
        "from datasets import Dataset\n",
        "dataset = Dataset.from_pandas(sample_df)"
      ],
      "metadata": {
        "id": "4bGiDyfASJC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gyXbdMVTMea",
        "outputId": "5f319eb1-3cc7-4d02-bb0b-7544a67800e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['dataset_id', 'original_text', 'rewrite_prompt', 'rewritten_text', 'prompt', 'token_count', 'token_count_rewrit', 'token_count_promp', '__index_level_0__'],\n",
              "    num_rows: 3000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenizing the dataset\n",
        "\n",
        "dataset = dataset.map(lambda samples: tokenizer(samples['prompt']), batched=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "abcf5217d2234f48864e23d215e73e16",
            "135b9ee2e0c94ac2a9e554e1fe8b5c48",
            "836100ca461a450f8c35c1c5460b7045",
            "9732ec2f70fb40ea90a9c11ab726a356",
            "41916236f7a84216b26a56f17ec3ef6a",
            "f334c9e38110425daa777c1898590b83",
            "59e1c32b70874317bb45d14fa99f1033",
            "15c093250c094ac88c200ba5c9b1328b",
            "c2c811db381947c7b1a367c07d4befd0",
            "1b94b95ce88349e7b4c4cddfa94d3cde",
            "78356ce6ee7c40fda943868fad9f31fc"
          ]
        },
        "id": "pZQwsHJVTXxx",
        "outputId": "32a463f8-4271-44f9-ca47-bffdb8f0ba78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "abcf5217d2234f48864e23d215e73e16"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9ESAFZyTi20",
        "outputId": "46eaec90-3ac7-4ceb-a107-d59bcf06e8ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['dataset_id', 'original_text', 'rewrite_prompt', 'rewritten_text', 'token_count', 'token_count_rewrit', 'prompt', 'token_count_promp', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
              "    num_rows: 3000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install trl\n",
        "\"\"\"\n",
        "Supervised fine-tuning (or SFT for short) is a crucial step in RLHF. In TRL we provide an easy-to-use API to create your SFT models and train them with few lines of code on your dataset.\n",
        "SFT allows popular instruction or conversational formats:\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"You are helpful\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
        "these formats can be passed to SFTTrainer using :\n",
        "tokenizer.apply_chat_template(chat, tokenize=False)\n",
        "or a custom formating function:\n",
        "\n",
        "def formatting_prompts_func(example):\n",
        "    output_texts = []\n",
        "    for i in range(len(example['question'])):\n",
        "        text = f\"### Question: {example['question'][i]}\\n ### Answer: {example['answer'][i]}\"\n",
        "        output_texts.append(text)\n",
        "    return output_texts\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model,\n",
        "    train_dataset=dataset,\n",
        "    formatting_func=formatting_prompts_func,\n",
        ")\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "eiEmdiMIcSLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "ZHAGMqa4b-nB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\"\"\"\n",
        "- gradient_accumulation_steps allows for lower memory usage; instead of calculating the gradients for the whole batch at once to do it in smaller steps, when enough gradients are accumulated we run the model’s optimization step.\n",
        "The reason to use gradient accumulation steps is when your whole batch size does not fit on one GPU, the training becomes slower\n",
        "- SFT Trainer also accepts PEFT config to pass the LoRa configuration with: peft_config=peft_config\n",
        "\"\"\"\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    dataset_text_field= \"prompt\",\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=2,\n",
        "        warmup_steps=100,\n",
        "        max_steps=200,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=1,\n",
        "        output_dir='outputs',\n",
        "\n",
        "\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        "    train_dataset=dataset,\n",
        "    peft_config=config,\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_length=600,\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "#The Regular HuggingFace trainer can also be used if wanted\n",
        "\"\"\"trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=2,\n",
        "        warmup_steps=100,\n",
        "        max_steps=200,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=1,\n",
        "        output_dir='outputs',\n",
        "        max_length=600\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\"\"\"\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0eb33701c684480dbfef4e10e93669f2",
            "db42d1ebd03743378b324ed5a6fd9331",
            "a568927b607545d6811c5f5b9e293750",
            "42c51d785dae435e9a9cbcb4ec03504c",
            "8eb86828c6a54f609f4cdc873fd52676",
            "bf2921440bef47c59ff627af48bbb7f9",
            "d9b1e32822ed4d518b626a3b3d8b4c3e",
            "e02dbcfc109f4313bd8f8982886b5102",
            "dccd42f6432643eabf837b760ac597bb",
            "390a5bd7d5bb4bcb893ffd2b222fed00",
            "80dd450796424e1ba7055104cc0a3082"
          ]
        },
        "id": "4DsH0IQzTiB1",
        "outputId": "698fc2a0-5019-4405-a838-ffb0c5177045"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0eb33701c684480dbfef4e10e93669f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:317: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [200/200 22:33, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.865300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.792400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.772800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.644000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.944900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.839800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.821800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.024300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.534300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.508600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.734300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.532500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.798900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.687500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.726200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.516500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.719500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.513500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.621600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.029100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.510600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.697900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.700600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.473400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.492300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.661600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.535000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.789100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.531200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.303000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>1.918800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.405700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>1.271500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.498500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.643000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>1.595200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>1.638800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>1.270500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.619300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.090100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>1.506100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>1.632300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>1.875900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>1.428300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>1.512700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>1.309700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>1.503300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>1.313300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>1.416000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.673500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>1.523700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>1.506700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>1.139500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>1.182000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>1.542100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>1.337300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>1.282600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>1.466600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>1.434300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.139200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>1.192800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>1.188700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>1.261900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>1.313800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>1.336900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>1.350500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>1.209800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>1.261200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>1.199800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.131700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>1.192800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>1.703400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>1.379300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>1.111200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>1.086300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>1.277800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>1.177500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>1.237500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>1.537800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.773400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>1.547800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>1.305300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>0.963000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.948100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>1.441000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>1.029000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>1.193700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>1.210500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>1.032100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.959100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.853200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>1.316700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>1.146100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>1.098100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.919300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>1.268600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>1.533100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>1.505200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>1.133600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.058700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>1.188900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>1.821600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>1.139700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>1.350200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>1.202900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>1.468300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>1.286800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>1.261000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>1.489300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.600400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>1.602200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>1.802800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>1.430900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>1.439000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>1.851400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>1.529400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>2.137600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>1.653200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>1.428300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.684800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>1.921800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>1.673000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>1.369600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>1.715800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>1.769000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>1.891900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>1.607200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>1.544900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>1.569900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.720200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131</td>\n",
              "      <td>1.614100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>1.970900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133</td>\n",
              "      <td>1.574600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>1.728900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>1.670700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>1.682500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137</td>\n",
              "      <td>1.513000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>1.962600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139</td>\n",
              "      <td>1.309300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.726000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141</td>\n",
              "      <td>1.953300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>1.657400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143</td>\n",
              "      <td>1.701900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>1.752300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>1.756500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146</td>\n",
              "      <td>1.707000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147</td>\n",
              "      <td>1.985700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>1.735500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149</td>\n",
              "      <td>1.610300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.549400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151</td>\n",
              "      <td>1.931300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152</td>\n",
              "      <td>1.907400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153</td>\n",
              "      <td>1.762100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>1.938900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>1.911800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156</td>\n",
              "      <td>2.020000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157</td>\n",
              "      <td>1.774000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158</td>\n",
              "      <td>1.878800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159</td>\n",
              "      <td>1.700200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.797700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>161</td>\n",
              "      <td>1.457200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162</td>\n",
              "      <td>1.579800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>163</td>\n",
              "      <td>1.954900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164</td>\n",
              "      <td>1.771000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>1.476600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166</td>\n",
              "      <td>1.718400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>167</td>\n",
              "      <td>1.779900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>1.483600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>169</td>\n",
              "      <td>1.951900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>1.787400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>1.905900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172</td>\n",
              "      <td>1.328600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>173</td>\n",
              "      <td>1.511200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>174</td>\n",
              "      <td>1.847700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>1.749600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>176</td>\n",
              "      <td>1.967800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>177</td>\n",
              "      <td>1.709700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>178</td>\n",
              "      <td>1.756000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>179</td>\n",
              "      <td>1.852800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>1.793900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>181</td>\n",
              "      <td>1.506900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>182</td>\n",
              "      <td>1.805500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>183</td>\n",
              "      <td>2.079900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>184</td>\n",
              "      <td>1.626800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>185</td>\n",
              "      <td>1.523400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>186</td>\n",
              "      <td>1.840800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>187</td>\n",
              "      <td>1.893400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>188</td>\n",
              "      <td>1.389000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>189</td>\n",
              "      <td>1.692400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>1.659700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>191</td>\n",
              "      <td>1.684500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>192</td>\n",
              "      <td>1.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>193</td>\n",
              "      <td>1.608300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>194</td>\n",
              "      <td>1.579700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>195</td>\n",
              "      <td>1.396700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>1.919500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>197</td>\n",
              "      <td>1.944400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>198</td>\n",
              "      <td>1.882200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>199</td>\n",
              "      <td>1.953600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.628700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=200, training_loss=1.5546031677722931, metrics={'train_runtime': 1363.5433, 'train_samples_per_second': 0.587, 'train_steps_per_second': 0.147, 'total_flos': 4392776392212480.0, 'train_loss': 1.5546031677722931, 'epoch': 0.27})"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[4][\"prompt\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "H7ROXseWjSIY",
        "outputId": "2e1e786f-9c4e-48af-a506-f053f5f1d301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n<start_of_turn>user\\nYou are a reverse prompt engineer. Read the following \"Original Text\" and create the correct \"Instruction\" that instructs a LLM to generate the the \"Rewriten Text\" accurately.\\n\\n Original Text:\\nElizabeth Edwards, 49, and her 13-year-old daughter Katie were found dead in Spalding in April.\\nThe 15-year-old denies murder, but has admitted manslaughter on the grounds of diminished responsibility, at Nottingham Crown Court.\\nA boy, also 15, has already pleaded guilty to murder.\\nNeither teenager can be named because of their ages.\\nRead more about this and other stories from across Lincolnshire\\nOn the second day of the trial a transcript of the girl\\'s police interview was read.\\nShe told officers she and her co-accused, who was 14 at the time, had planned the killing of Elizabeth and Katie Edwards in the days before their deaths on the evening of 13 April.\\nShe also said they had attempted to carry out the killings on two other occasions.\\nThe jury heard her tell police her co-accused had stabbed Ms Edwards in the neck and then smothered her before going into 13-year-old Katie\\'s room and stabbing her.\\nThe trial continues.\\n\\n Rewriten Text:\\n**Urban Planner\\'s Vision for Green City**\\n\\nIn envisioning the future of our city, I am emboldened by the spirit of innovation and sustainability that permeates the hearts of our residents. With meticulous planning and unwavering commitment, we can craft a greener, healthier, and more resilient metropolis.\\n\\nImagine a city where lush greenery intertwines with vibrant urban landscapes, where clean energy powers our homes and businesses, and where sustainable transportation options connect every corner. This is the urban canvas we can create together.\\n\\nTo achieve this vision, we must invest in green infrastructure—bicycle paths, pedestrian promenades, and interconnected parks—that not only enhance our quality of life but also reduce traffic congestion and contribute to a healthier community.\\n\\nIn tandem with greening our streets, we must embrace renewable energy technologies, harnessing solar power, wind turbines, and other sustainable sources to reduce our reliance on fossil fuels. To foster this transition, we must incentivize green energy adoption and invest in efficient infrastructure.\\n\\nTransportation, a cornerstone of any city, presents a pivotal opportunity for transformation. By investing in electric vehicles, bike-sharing programs, and improved public transportation networks, we can encourage our residents to ditch their cars and embrace greener alternatives.\\n\\nCreating a green city requires a multifaceted\\n<end_of_turn>\\n<start_of_turn>model\\n\\nInstruction:\\nChange the text into an urban planner\\'s vision for a green city\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gen_kwargs = {\"min_length\": -1, \"top_p\": 1.0, \"pad_token_id\": tokenizer.eos_token_id}"
      ],
      "metadata": {
        "id": "54nz5g5WnvZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Some generation kwargs are use to improve the search in the inference step.\n",
        "top_k:With `top_k` sampling, the output gets restricted the k most likely tokens\n",
        " do_sample: With sampling, the output is unexpected -- sometimes too unexpected.\n",
        "For more information on generation strategies https://huggingface.co/docs/transformers/internal/generation_utils.\n",
        "Added gen_kwargs to show both options on how they can be passed onto the generate function.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "batch = tokenizer(dataset[4][\"prompt\"].split(\"Instruction:\")[0], return_tensors='pt')\n",
        "batch.to(\"cuda\")\n",
        "with torch.cuda.amp.autocast():\n",
        "  output_tokens = model.generate(**batch,  top_k=4, do_sample=True, max_new_tokens=50, **gen_kwargs)\n",
        "\n",
        "print('\\n\\n', tokenizer.decode(output_tokens[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKYM-_i1jQp2",
        "outputId": "c68310d2-8b3b-4af8-c072-3d5a6d868901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " \n",
            "user\n",
            "You are a reverse prompt engineer. Read the following \"Original Text\" and create the correct \"Instruction\" that instructs a LLM to generate the the \"Rewriten Text\" accurately.\n",
            "\n",
            " Original Text:\n",
            "Elizabeth Edwards, 49, and her 13-year-old daughter Katie were found dead in Spalding in April.\n",
            "The 15-year-old denies murder, but has admitted manslaughter on the grounds of diminished responsibility, at Nottingham Crown Court.\n",
            "A boy, also 15, has already pleaded guilty to murder.\n",
            "Neither teenager can be named because of their ages.\n",
            "Read more about this and other stories from across Lincolnshire\n",
            "On the second day of the trial a transcript of the girl's police interview was read.\n",
            "She told officers she and her co-accused, who was 14 at the time, had planned the killing of Elizabeth and Katie Edwards in the days before their deaths on the evening of 13 April.\n",
            "She also said they had attempted to carry out the killings on two other occasions.\n",
            "The jury heard her tell police her co-accused had stabbed Ms Edwards in the neck and then smothered her before going into 13-year-old Katie's room and stabbing her.\n",
            "The trial continues.\n",
            "\n",
            " Rewriten Text:\n",
            "**Urban Planner's Vision for Green City**\n",
            "\n",
            "In envisioning the future of our city, I am emboldened by the spirit of innovation and sustainability that permeates the hearts of our residents. With meticulous planning and unwavering commitment, we can craft a greener, healthier, and more resilient metropolis.\n",
            "\n",
            "Imagine a city where lush greenery intertwines with vibrant urban landscapes, where clean energy powers our homes and businesses, and where sustainable transportation options connect every corner. This is the urban canvas we can create together.\n",
            "\n",
            "To achieve this vision, we must invest in green infrastructure—bicycle paths, pedestrian promenades, and interconnected parks—that not only enhance our quality of life but also reduce traffic congestion and contribute to a healthier community.\n",
            "\n",
            "In tandem with greening our streets, we must embrace renewable energy technologies, harnessing solar power, wind turbines, and other sustainable sources to reduce our reliance on fossil fuels. To foster this transition, we must incentivize green energy adoption and invest in efficient infrastructure.\n",
            "\n",
            "Transportation, a cornerstone of any city, presents a pivotal opportunity for transformation. By investing in electric vehicles, bike-sharing programs, and improved public transportation networks, we can encourage our residents to ditch their cars and embrace greener alternatives.\n",
            "\n",
            "Creating a green city requires a multifaceted\n",
            "\n",
            "model\n",
            "\n",
            "Instruction:\n",
            "Rewrite this as a visionary urban planner's plan for the future of the city.\n",
            "\n",
            "\n",
            "\n",
            "model\n",
            "\n",
            "Instruction:\n",
            "Imagine this as if it were a visionary urban planner's plan for the future\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result= tokenizer.decode(output_tokens[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "YtbUwgpHl_gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# Use regular expression to extract text between \"\\nInstruction:\\n\" and the next \"\\n\"\n",
        "instruction_text = re.search(r'\\nInstruction:\\n(.*?)\\n', result, re.DOTALL)\n",
        "if instruction_text:\n",
        "    instruction_text = instruction_text.group(1).strip()\n",
        "\n",
        "print(instruction_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcpc9fRSsZsX",
        "outputId": "c7582386-334e-462c-9e55-cda18f139f57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rewrite this as a visionary urban planner's plan for the future of the city.\n"
          ]
        }
      ]
    }
  ]
}