# LLM-Promp-Recovery-Kaggle
Submission for the Kaggle competition: LLM prompt recovery Gemini
## Data
No training data is provided in this competition; in other words, we can use any openly available datasets for this competition. In this notebook, we will use two external datasets that utilize the Gemma 7B model to transform texts using prompts.

### Data Format:

These datasets includes:

original_text: Input text/essay that needs to be transformed.
rewrite_prompt: Prompt/Instruction that was used in the Gemma LM to transform original_text. This is also our target for this competition.
rewritten_text: Output text that was generated by the Gemma model.

## The Model

The competition is based on the use of Gemma.
### Gemma
Gemma is a family of lightweight open models created by Google DeepMind. This family of models are based on the Gemini models. Google released two models:
- Gemma 2b (18 layers, 32768 feedfoward hidden dimensions)
- Gemma 7b (28 layers, 49152 feedfoward hidden dimensions)

Both Models are available as base models or instruction tunned checkpoints with 256128 vocab size. 
Google research states the models outperform LLama 2 7B and Mistral 7B on several Benchmarks.

The model architecture is based on the transformer decoder using multi-query attention, multi-head attention, RoPe Embeddings, GeGlu activations, SentencePiece tokenization.
The formatting used in Supervised Fine tuning and reinforcement learning with human feedback is:

User: <start_of_turn>user
Knock knock.<end_of_turn>
<start_of_turn>model
Model: Whoâ€™s there?<end_of_turn>
User: <start_of_turn>user
Gemma.<end_of_turn>
<start_of_turn>model
Model: Gemma who?<end_of_turn>

More information on:
https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf


## The submissions

This repository has two .ipynb files:
1. Training and inference using Pytorch and HuggingFace
   - Use of HuggingFace transformers, peft, and dataset libraries.
   - Use of SFTTrainer
   - Memory efficient techniques are applied, such as load in 8 bit, gradient_accumulation_steps, LoRa.
   - Generation techniques to increase the quality of inference: top_k, do_sample 
3. Training and inference using Keras and Kaggle
  - Use of LoRa with Rank 4
  - CosineDecay 
